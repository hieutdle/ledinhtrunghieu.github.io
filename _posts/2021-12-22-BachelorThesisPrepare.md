---
layout: post
author: ledinhtrunghieu
title: Preparation for Bachelor Thesis
---

**Data lake**

**Goal**

The idea is to create an optimized data lake which will enable users to analyze data. The main goal of this project is to build an end to end data pipeline which is capable to work with big volumes of data. We want to clean, transform and load the data to our optimized data lake on S3. The data lake will consist of logical tables partitioned by certain columns to optimize query latency. This will allow analytics team to find insights.


**Technologies**

We are going to store our data lake on Amazon S3, which is is an object storage service that offers industry-leading scalability, data availability, security, and performance. S3 is a perfect places for storing our data partitioned and grouped in files. It has low cost and a lot of flexibility.

For our ETL process we are using Apache Spark running on an EMR cluster on AWS. Spark provides great performance because it stores the data in-memory shared across the cluster.

Finally, to orchestrate everything, we are going to build a data pipeline using Apache Airflow. Airflow provides an intuitive UI where we can track the progress and bottlenecks of our pipelines.

**Data warehouse**

Building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for analytics team to continue finding insights.

**Data Lake:** People focus a lot on the unstructured aspect of the data lake. It should be good at storing and processing unstructured data, but the lake is much more than that. **This is the location where you put data that you have yet to prove the value of.** If I'm at work and brainstorm a new external dataset that I can collect from scraping the web, then the data lake is where I would want to land that data. Then I can merge that data with content from the enterprise data warehouse to prove out how valuable it is to different areas of the business. **Proving the value of data is a critical role of the data lake.**

**Data Warehouse**: Already hinted at this a bit, but this is **where you store your already valuable, enterprise data that is generated by business processes and consumed in reports**. It should be reasonably governed based on the sensitivity of the data as well as how critical accuracy is for down-stream applications. Generally you want this to be in a format that is easy for business users / power users to consume. **Star schemas and OLAP cubes are best for this. The focus here is on simplicity, ease-of-use, and the ability to adapt to new sources in the future.** Focus should be given to business processes. What are you trying to measure? These are Facts that go in Fact tables. What are you trying to group these facts by? These are Dimensions that go in Dimension tables. Keep it simple and resist the urge to normalize (should look very different from an operational database that you'd find connected to a web application or transactional system).

https://www.reddit.com/r/datascience/comments/88zcb0/whats_the_difference_between_a_data_lake_data/

# 1. OLAP vs OLTP

## 1.1 OLTP

* Online Transaction Processing
* The most traditional processing system. 
* Able to manage transaction-oriented applications and can be characterized by a large number of short, atomic database operations, such as inserts, updates, and deletes, 
* Common in your day-to-day application. 

Examples: 
1. Online banking and e-commerce applications.
2. Find the price of a book
3. Update latest customer transaction
4. Keep track of employee hours

Purpose: support daily transactions
Design: application-oriented
Data: up-to-date, operational
Size: snapshot, gigabytes
Queries: simple transactions & frequent updates
Users: thousands


## 1.2 OLAP

Online Analytical Processing, manages historical or archival data. It is characterized by a relatively low volume of transactions. OLAP systems are typically used for analytical purposes — to extract insights and knowledge from bulk data, merged from multiple sources. Unlike OLTP, the goal for OLAP systems is to have a limited amount of transactions, each consisting of mostly bulk reads and writes. Data warehouses are the typical infrastructure to maintain these systems.

Example: 
1. Calculate books with best profit margin
2. Find most loyal customers
3. Decide employee of the month

Purpose: report and analyze data 
Design: Object-oriented (certain subject that’s under analysis) 
Data: consolidated, historical (over a large period of time, consolidated for long-term analysis.) 
Size: archive, terabytes (large amount) 
Queries: complex, aggregate queries & limited updates Users: hundreds

<img src="/assets/images/20211222_BachelorThesis/pic1.png" class="largepic"/>

## 1.3 Row vs Columnar 

Row-oriented databases are commonly used in OLTP systems, whereas columnar ones are more suitable for OLAP.

Row-oriented databases store the whole row in the same block, if possible. Columnar databases store columns in subsequent blocks.

<img src="/assets/images/20211222_BachelorThesis/pic2.png" class="largepic"/>

<img src="/assets/images/20211222_BachelorThesis/pic3.png" class="largepic"/>

**Perfomance**

For the analytical query:

What is the average age of males?

We would have these possibilities:

Row wise DB -> Has to read all the data in the database — 100GB to read.
Columnar DB -> Has to read only the columns age and gender — 2GB to read.

But Row database performs fast multiple read, write, update, delete operations	

https://dzone.com/articles/the-data-processing-holy-grail-row-vs-columnar-dat

# 2. Star vs Snowflake schema

## 2.1. Star Schema
**Dimesion modeling: Star schema**

**Fact tables**
* Decided by business use-case 
* Holds records of a metric 
* Changes regularly
* Connects to dimensions via foreign keys

**Dimension tables**
* Holds descriptions of attributes 
* Does not change as often

**Example**
* Supply books to stores in USA and Canada
* Keep track of book sales

**Star schema example**

<img src="/assets/images/20210507_DatabaseDesign/pic11.png" class="largepic"/>


## 2.2. Snowflake schema 

**Snowflake schema (an extension)**

<img src="/assets/images/20210507_DatabaseDesign/pic12.png" class="largepic"/>

Snow ake schemas: more than one dimension. Because dimension tables are **normalized**

**What is normalization?**
* Database design technique
* Divides tables into smaller tables and connects them via relationships 
* **Goal**: reduce redundancy and increase data integrity
**Identify repeating groups of data and create new tables for them**

**Book dimension of the star schema**
Most likely to have repeating values:
* Author 
* Publisher
* Genre

<img src="/assets/images/20210507_DatabaseDesign/pic13.png" class="largepic"/>

**Book dimension of the snowflake schema**

<img src="/assets/images/20210507_DatabaseDesign/pic14.png" class="largepic"/>

**Store dimension of the star schema**

<img src="/assets/images/20210507_DatabaseDesign/pic15.png" class="largepic"/>

* City
* State 
* Country

**Store dimension of the snowflake schema**

<img src="/assets/images/20210507_DatabaseDesign/pic16.png" class="largepic"/>

<img src="/assets/images/20210507_DatabaseDesign/pic17.png" class="largepic"/>

## 2.3. Which schema is better for performance?
The Star schema is in a more de-normalized form and hence tends to be better for performance. Along the same lines the Star schema uses less foreign keys so the query execution time is limited. In almost all cases the data retrieval speed of a Star schema has the Snowflake beat.

## 2.4. Which schema is better for readability?
The Star schema is easier for readability because its query structure is not as complex, on the other hand the Snowflake has a complex query structure and is tougher for readability and implementing changes. The changes to be implemented can be tougher to put into a Snowflake schema because of the tendency to have a lot of joins in the query structure. The Star schema on the other hand uses less joins and tends to have more data redundancy. So for readability the schema to go with would be the star schema.

## 2.5. Which schema is better for maintainability?
Maintainability for a data warehouse is heavily dependent on the amount of redundant data. The more redundancies the more places the maintenance needs to take place. Out of the two schemas the Snowflake has the least data redundancies so is hence the more maintainable choice.

## 2.6. Snowflake vs Star Schema
Now comes a major question that a developer has to face before starting to design a data warehouse. Snowflake or Star schema? We’ve gone over the difference and the choice needs to be made on a case by case basis, but two important factors outside of the above, which are more personal choices, are the number of dimensions in your data and the size of the data.

If the data is relatively small and the end result is more of a DataMart than a data warehouse, then the choice tends to lean towards Star schema. Along the same line, if the relationships inside the data are simple and don’t have many to many relationships then the choice tends to lean towards Star Schema. On the other hand, if you are building a bigger solution with many to many relationships then going with the Snowflake is your best bet.

Another thing that needs to be considered is the number of dimensions in your dimension table. If a single dimension requires more than one table, it’s better to use the Snowflake schema. For example, a star schema would use one date dimension but a Snowflake, can have Dimension date tables that extends out to dimension day of the week, quarter, month…etc. If these branches or snowflakes are needed than the Star schema isn’t the way to go.

# 3. Cloud

Companies can process data in their own data center, often on **premises**. 

Server on **premises**:
* Bought (racks of servers)
* Need space (a room to store)
* Inconvenient (if we move offices, we have to transport servers without losing service)
* Electrical and maintenance cost

Processing tasks can be more or less intense, and don't happen continuously. 
* Companies would need to provide enough processing power for peak moments
* Processing power unused at quieter times (at quieter times, much of the processing power would remain unused)

It's avoiding this waste of resources that makes **cloud** computing so appealing.

Server on **cloud**:
* Rented server (the rent is cheap)
* Don't need space (don't need a room to store)
* Use the resources we need, at the time we need them
* The closer the server is to the user, the less latency they will experience when using our application

**Cloud computing for data storage**:
* Database reliability: Running a data-critical company, we have to prepare for the worst. A fire can break out in an on-premises data center. To be safe, we need to replicate our data at a different geographical location. 
* Risk with sensitive data: If your company manipulates sensitive or confidential data, there is a risk associated with someone else hosting it, and government surveillance. 

With the advent of big data, companies specializing in these kinds of issues, called cloud providers, were born.
The three big players, in decreasing order of market share, are Amazon Web Services, Microsoft Azure, and Google Cloud.

Storage services allow you to upload files of all types to the cloud
Computation services allow you to perform computations on the cloud.
Database services is a database that typically runs on the cloud.

<img src="/assets/images/20210422_DEForEveryone/pic10.png" class="largepic"/>

# 4. Amazon Web Services





# 5. Data processing - Batch, MapReduce, Streaming

# 6. Pipeline / Workflow Management

# 7. Tools


# 7.1 Custom ETL
Many companies use general purpose programming languages to write their own ETL tools. This approach has the greatest flexibility, but also requires the most effort. This approach also requires users to perform their own maintenance, build their own documentation, test and perform ongoing development. Users of custom ETL often find it difficult to source help from people outside of their own team.

SQL

Python

# 7.2 ETL Cloud Services

AWS EMR. Elastic MapReduce (EMR) is the Hadoop offering provided by AWS. Companies running on AWS who like to use Hadoop for ETL or ELT may use EMR. As with any Hadoop distribution, there are several tools available to perform ETL, including Hive and Spark. 



# 7.3 Open Source ETL Tools

Over the past 10 years, software developers have created several open source ETL products. These products are free to use. Their source code is also freely available, which allows you to extend or enhance their capabilities. These tools vary significantly in quality, integrations, ease of use, adoption and availability of support. Like the enterprise ETL tools, many of these open source ETL tools provide a graphical interface for designing and executing pipelines.

Dremio

Pentaho Data Integration (PDI)








ETL and ELT

What Is ETL?
Historically, data warehouses were optimized to query and read large datasets fast for accurate business intelligence. This made data warehouses good at processing read operations (SELECT, WHERE, etc.). However, the cost of building and setting up a data warehouse – in terms of buying hardware, licensing software, and developing and maintaining the system – was a multi-million-dollar undertaking. To save on costs, developers would only load cleaned, transformed, and aggregated data into their warehouses – and for greater efficiency, they would remove any data that wasn’t necessary for the analysis.   

To prepare data like this, organizations needed to extract data from different databases, transform it into a unified format, and remove unnecessary information before loading it into the warehouse. This gave rise to ETL (extract, transform, load) tools, which prepare and process data in the following order:

Extract raw, unprepared data from source applications and databases into a staging area. Data from different sources has its own 
Transform and aggregate the data with SORT, JOIN, and other operations while it is in the staging area.
Load data into the warehouse.



What Is ELT?
Most organizations continue to rely on ETL for data integration, but the need for preload transformations has changed with the rise of high-performance, cloud-based data warehouses (like Redshift, Azure, BigQuery, and Snowflake). 

Modern cloud data warehouses have the processing capability to efficiently manage write operations on large data sets. In fact, cloud data warehouses are so fast at processing data that they have rendered ETL unnecessary for many use-cases. This has ultimately given rise to a new data integration strategy, ELT, which skips the ETL staging area for speedier data ingestion and greater agility. ELT sends raw, unprepared data directly to the warehouse and relies on the data warehouse to carry out the transformations post-loading.  

ELT tools prepare and process data in the following order:

Extract raw, unprepared data from source applications and databases.
Load the unprepared data into the warehouse.
Use the data warehouse to process transformations when required.
The main point to remember with ELT is that data transformations happen within the data warehouse itself, which typically bypasses the need for a staging server. In this respect, the data warehouse contains both raw and transformed data inside it. 

ELT offers an excellent way to collect and store large amounts of raw, unstructured data. At the same time, ELT doesn’t give you the option of removing PHI, PII, and other sensitive data before loading it into the data warehouse. Therefore, it isn’t perfect from the perspectives of data security, compliance, and data quality. Simply put, sacrificing security and compliance in the name of speed and flexibility just isn’t an option for many businesses, which is why most businesses continue to rely on ETL or they select the hybrid approach of ETLT (more on this later).

ETL Advantages and Use-Cases
The advantages of ETL apply to the following scenarios:

Data compliance and security: Whether your organization adheres to industry-specific data compliance standards – like SOC 2, GDPR, CCPA, and HIPAA – or a data compliance standard of your own, it may be necessary to remove, mask, or encrypt PHI (protected health information), PII (personally identifiable information), and other data before moving it to the data warehouse. An ETL strategy that transforms data before loading can achieve this, but an ELT strategy can’t. For instance, with ELT, SysAdmins may have access to sensitive information in logs even if the data warehouse transforms it after loading. This makes ETL more suitable when data compliance is a concern.
Managing large datasets: Even though a modern cloud data warehouse can handle virtually any size dataset, you might not want to pay extra data warehousing fees for this service. Instead, you can use an ETL solution to remove unnecessary or redundant data in-pipeline to reduce your data storage expenses. In contrast, an ELT solution could load a lot of unnecessary data into the data warehouse.
Data warehouse processing fees: Many ELT tools offer lower upfront prices, but this is only because ELT shifts data processing costs over to the data warehouse. In the end, ELT doesn’t bypass the cost of processing data transformations. For this reason, many organizations choose ETL as a more cost-efficient way to perform data transformations. 

Data quality: ETL ensures data quality by managing data transformations in batches and by standardizing data formats to prevent unintended data corruption.


ELT Advantages and Use-Cases
The advantages of ELT apply to the following use-cases:

Rapid data ingestion: ELT allows you to quickly add new data sources and ingest any kind of raw data immediately without the data passing through a staging server. These advantages make ELT ideal for use-cases that require real-time streaming and rapid data ingestion.


Ingest and save data that you might need later: Traditional ETL involves the aggregation of data in a certain way, which requires you to throw out data. But ELT lets you save all data in the data warehouse – even data that you don’t have a use for now, but could be useful later.


Transform only the data you need for a specific analysis: ELT allows you to load raw data into the warehouse for storage purposes, and transform only specific data in the way that best supports a specific type of analysis. This slows down the process of reading and analyzing the information because each analysis transforms the data from scratch. However, for certain use-cases, it offers the flexibility to easily change the business logic of your data on the fly as your requirements change, or as you learn more about the data.  


More individuals have the skills to code in-warehouse transformations: In-warehouse transformations are usually coded in basic SQL. The prevalence of SQL knowledge makes the process of coding ELT transformations more accessible for a larger number of developers, i.e., it’s easier and less expensive to find developers who can manage this task.



What Is ETLT? How ETLT Merges the Best of ETL and ELT
In the age of big-data analytics, ELT offers tremendous advantages because rapid ingestion gives BI solutions access to more data faster – including raw and unstructured information. ELT also brings the flexibility to change analytics strategies on the fly. However, the limitation of an ELT-only strategy is that it cannot always meet data security and compliance requirements – especially those that require you to mask, remove, or encrypt PHI and PII data before moving it into a data warehouse. 

Whether you need to adhere to national or industry data compliance standards – or a security standard of your own – your organization simply might not be able to sacrifice any degree of security and compliance in the name of faster analytics. Thus, we come to the need for ETLT (extract, transform, load, transform). 

ETLT is a “best of both worlds” approach to data integration that (1) speeds up data ingestion while (2) ensuring data quality and securing sensitive data in accordance with industry compliance standards. ETLT uses the following data integration pattern:

Extract the raw, unprepared data from source applications and databases and load it into a staging area.
Transform data “lightly” while it’s in the staging area (usually to remove/mask/encrypt PHI, PII, or other sensitive data). The first transformation stage only applies to one data source at a time. These transformations are fast and simple because they transform each source independently of other sources. There is no attempt to integrate two data sources into one until after loading. Transformations for this first stage relate to data formats, data cleansing, and masking/removing sensitive data for compliance purposes.
Load the prepared data into the data warehouse.
Transform and integrate data more completely within the data warehouse using the data warehouse to process those transactions. The second transformation stage relates to integrating multiple data sources and other transformations that apply to data from multiple sources at the same time.

https://www.integrate.io/blog/what-is-etlt/

ETL is generally used when we transform all the data before storing it anywhere. In ELT, you first store the data and transform when needed. ELT is good when you the transform is not well defined or you want create the data latter with different transform logic.

https://blog.panoply.io/etl-vs-elt-the-difference-is-in-the-how 
Has:image


The reason for ELT type things is people aren't exactly sure how they want to use data. ELT allows them to keep granular datasets and decide later how they want to aggregate them.





