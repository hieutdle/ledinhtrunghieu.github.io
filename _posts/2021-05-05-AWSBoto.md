---
layout: post
author: ledinhtrunghieu
title: Lesson 14 - Introduction to AWS Boto in Python
---

# 1. Putting Files in the Cloud

Learning how AWS works to creating S3 buckets and uploading files to them. 

## 1.1. Intro to AWS and Boto3

**What is Boto3**

To interact with AWS in Python, we use the Boto3 library. 

```python
import 

s3 = boto3.client('s3',
                  region_name='us-east-1', 
                  aws_access_key_id=AWS_KEY_ID, 
                  aws_secret_access_key=AWS_SECRET)

response = s3.list_buckets()


```

We initialize the client with an AWS service name, region, key and secret. The service name can be any of the 100+ available AWS services. The region is the geo region where our resources are located. **Key** and **secret** are like a username and password to an AWS account.

**Creating keys with IAM**

<img src="/assets/images/20210505_AWSBoto/pic1.png" class="largepic"/>

* Creating an account at aws.amazon.com gives us access to the AWS Console. Anything we do here to manage our web services, we can do in Boto3 in Python.
* To log into the console, we use the username/password we signed up with. This is the root user.
* To create the key/secret for Boto3, we are going to use **IAM** or **Identity Access Management Service**. We create IAM sub-users to control access to AWS resources in our account. Credentials - or the key / secret combo are what authenticate IAM users.

**AWS services**
* **S3** or **Simple Storage Service** lets us store files in the cloud.
* **SNS** or **Simple Notification Service** lets us send emails and texts to alert subscribers based on events and conditions in our data pipelines.
* **Comprehend** performs sentiment analysis on blocks of text.
* **Rekognition** to extracts text from images and look for cats in a picture
* **RDS**, **EC2**, **Lambda** and other common services.


**Multiple clients**
```python
# Generate the boto3 client for interacting with S3 and SNS
s3 = boto3.client('s3', region_name='us-east-1', 
                         aws_access_key_id=AWS_KEY_ID, 
                         aws_secret_access_key=AWS_SECRET)

sns = boto3.client('sns', region_name='us-east-1', 
                         aws_access_key_id=AWS_KEY_ID, 
                         aws_secret_access_key=AWS_SECRET)

# List S3 buckets and SNS topics
buckets = s3.list_buckets()
topics = sns.list_topics()

# Print out the list of SNS topics
print(topics)
```

## 1.2. Diving into buckets

**S3** lets us put any file in the cloud and make it accessible anywhere in the world through a URL

**S3 Components - Buckets**
* **Buckets** are like folders on our desktop. **Objects** are like files within those folders. 
* Buckets have their own permissions policies. 
* Can be configured to act as folders for a static website
* Can generate **logs** about their own activity and write them to a different bucket.
* **Contain Object**


**S3 Components - Objects**
<img src="/assets/images/20210505_AWSBoto/pic2.png" class="largepic"/>
An object can be anything - an image, a video file, CSV or a log file.

**What can we do with buckets**
* Create Bucket
* List Buckets 
* Delete Bucket

```python
import boto3
s3 = boto3.client('s3', 
                  region_name='us-east-1',
                  aws_access_key_id=AWS_KEY_ID, 
                  aws_secret_access_key=AWS_SECRET)
```

**Creating a Bucket**
```python
bucket = s3.create_bucket(Bucket='gid-requests')
```
We call the client's create_bucket method, passing the bucket name as the argument.


**List Buckets**
```python
bucket_response = s3.list_buckets()
```
When S3 responds, it will give us some additional response metadata. it will include a dictionary under the Buckets key. 

**Get Buckets Dictionary**
```python
buckets = bucket_response['Buckets'] 
print(buckets)
```
<img src="/assets/images/20210505_AWSBoto/pic3.png" class="largepic"/>
We can see our new bucket name and the time that it was created. Now that we have this dictionary, we can run it through a for loop and perform an operation on multiple buckets.

**Deleting Buckets**
```python
response = s3.delete_bucket('gid-requests')
```

**Practice**

```python
import boto3

# Create boto3 client to S3
s3 = boto3.client('s3', region_name='us-east-1', 
                         aws_access_key_id=AWS_KEY_ID, 
                         aws_secret_access_key=AWS_SECRET)

# Create the buckets
response_staging = s3.create_bucket(Bucket='gim-staging')
response_processed = s3.create_bucket(Bucket='gim-processed')
response_test = s3.create_bucket(Bucket='gim-test')

# Print out the response
print(response_staging)

# Get the list_buckets response
response = s3.list_buckets()

# Iterate over Buckets from .list_buckets() response
for bucket in response['Buckets']:
  
  	# Print the Name for each bucket
    print(bucket['Name'])

# Delete the gim-test bucket
s3.delete_bucket(Bucket='gim-test')

# Get the list_buckets response
response = s3.list_buckets()

# Print each Buckets Name
for bucket in response['Buckets']:
    print(bucket['Name'])

# Get the list_buckets response
response = s3.list_buckets()

# Delete all the buckets with 'gim', create replacements.
for bucket in response['Buckets']:
  if 'gim' in bucket['Name']:
      s3.delete_bucket(Bucket=bucket['Name'])
    
s3.create_bucket(Bucket='gid-staging')
s3.create_bucket(Bucket='gid-processed')
  
# Print bucket listing after deletion
response = s3.list_buckets()
for bucket in response['Buckets']:
    print(bucket['Name'])
```

## 1.3. Uploading and retrieving files

**Buckets and Object**

<img src="/assets/images/20210505_AWSBoto/pic4.png" class="largepic"/>

**A Bucket**
<img src="/assets/images/20210505_AWSBoto/pic5.png" class="largepic"/>

* A bucket has a **name**
* **Name** is a string
* **Unique** name in all of S3.
* Contains **many object**.

**An Object**
<img src="/assets/images/20210505_AWSBoto/pic6.png" class="largepic"/>

* An object has a **key**
* **Name** is full path from bucket root 
* **Unique** key in the bucket
* Can only be in **one** parent bucket

**Uploading files**
<img src="/assets/images/20210505_AWSBoto/pic7.png" class="largepic"/>

```python
s3.upload_file( 
    Filename='gid_requests_2019_01_01.csv' 
    Bucket='gid-requests', 
    Key='gid_requests_2019_01_01.csv')
```

* The Filename is the local file path. 
* Bucket parameter takes the name of the bucket we are uploading to. 
* Key is what we want to name the object in S3. 

**Listing objects in a bucket**
<img src="/assets/images/20210505_AWSBoto/pic8.png" class="largepic"/>

```python
response = s3.list_objects( 
    Bucket='gid-requests', 
    MaxKeys=2,
    Prefix='gid_requests_2019_')

print(response)
```

* We can limit the response to two objects with the MaxKeys argument. 
* Another way to limit the response is to use the optional Prefix argument. Passing it will limit the response to objects that start with the string we provide.

<img src="/assets/images/20210505_AWSBoto/pic9.png" class="largepic"/>


**Getting object metadata**

If we want to know these things about a single object, we can use the client's `head_object` method, passing the `bucket name` and `object key`.

```python
response = s3.head_object( 
    Bucket='gid-requests',
    Key='gid_requests_2018_12_30.csv')

print(response)
```

<img src="/assets/images/20210505_AWSBoto/pic10.png" class="largepic"/>
Notice that because we are only working with one object, there is no Contents dictionary. The metadata is directly in the response dictionary.

**Downloading files**
<img src="/assets/images/20210505_AWSBoto/pic11.png" class="largepic"/>

```python
s3.download_file( 
    Filename='gid_requests_downed.csv', 
    Bucket='gid-requests', 
    Key='gid_requests_2018_12_30.csv')
```

**Deleting object**
<img src="/assets/images/20210505_AWSBoto/pic12.png" class="largepic"/>


```python
s3.delete_object( 
    Bucket='gid-requests',
    Key='gid_requests_2018_12_30.csv')
```

**Practice**
```python
# Upload final_report.csv with key 2019/final_report_01_01.csv
s3.upload_file(Bucket='gid-staging', 
               # Set filename and key
               Filename='final_report.csv', 
               Key='2019/final_report_01_01.csv')

# Get object metadata and print it
response = s3.head_object(Bucket='gid-staging', 
                       Key='2019/final_report_01_01.csv')

# Print the size of the uploaded object
print(response['ContentLength'])

# List only objects that start with '2018/final_'
response = s3.list_objects(Bucket='gid-staging', 
                           Prefix='2018/final_')

# Iterate over the objects
if 'Contents' in response:
  for obj in response['Contents']:
      # Delete the object
      s3.delete_object(Bucket='gid-staging', Key=obj['Key'])

# Print the keys of remaining objects in the bucket
response = s3.list_objects(Bucket='gid-staging')

for obj in response['Contents']:
  	print(obj['Key'])
```

# 2. Sharing Files Securely

Learn how to upload and share files securely, how set files to be public or private, and cap off by generating web-based reports!

## 2.1. Keeping objects secure
**Why care about permissions?**
AWS **defaults to denying permission**. That means if we upload a file, by default only our key and secret have access to it.

```python
df = pd.read_csv('https://gid-staging.s3.amazonaws.com/potholes.csv')
```
<img src="/assets/images/20210505_AWSBoto/pic13.png" class="largepic"/>
If we or our colleague tries to download a file with pandas, S3 won't let them. Even if it's in the same script that uploaded it!

If we initialize the s3 client with our credentials, it will work!

```python
# Generate the boto3 client for interacting with S3 
s3 = boto3.client('s3', region_name='us-east-1',
                        aws_access_key_id=AWS_KEY_ID, 
                        aws_secret_access_key=AWS_SECRET)

# Use client to download a file 
s3.download_file(
    Filename='potholes.csv', 
    Bucket='gid-requests', 
    Key='potholes.csv')
```

**AWS Permissions Systems**

<img src="/assets/images/20210505_AWSBoto/pic14.png" class="largepic"/>

There are 4 ways we can control permissions in S3:
* Use **IAM** to control users' access to AWS services, buckets, and objects: give permissions by attaching IAM policies to a user. IAM applies across all AWS services
* **Bucket** policies give us control on the bucket and the objects within it.
* **ACLs** or access control lists let us set permissions on specific objects within a bucket.
* **presigned URLs** let us provide temporary access to an object.

**ACLs**

**2 types of ACL: `private-read` and `public-read`.**

<img src="/assets/images/20210505_AWSBoto/pic15.png" class="largepic"/>


**Upload File**
```python
s3.upload_file(
    Filename='potholes.csv', Bucket='gid-requests', Key='potholes.csv')
```
Say we upload a file. By default, its ACL is 'private'. Let's set the ACL to 'public-read' using s3_put_object_acl method. Now anyone in the world can download this file.

**Set ACL to `public-read`**
```python
s3.put_object_acl(
    Bucket='gid-requests', Key='potholes.csv', ACL='public-read')
```

**Set ACL on upload: Upload file with `public-read` ACL**

```python
s3.upload_file( 
    Bucket='gid-requests', 
    Filename='potholes.csv', 
    Key='potholes.csv',
    ExtraArgs={'ACL':'public-read'})
```
We pass a dictionary with key ACL and value 'public-read' to the ExtraArgs parameter.

**Accessing public objects**
**S3 Object URL Templete**
```
https://{bucket}.s3.amazonaws.com/{key}
```
**URL for Key=`'2019/potheles.csv'`**
```
https://gid-requests.s3.amazonaws.com/2019/potholes.csv
```

**Generating public object URL**

* **Generate Object URL String**
```python
url = "https://{}.s3.amazonaws.com/{}".format( 
    "gid-requests",
    "2019/potholes.csv")
```
* **`'https://gid-requests.s3.amazonaws.com/2019/potholes.csv'`**
```python
# Read the URL into Pandas 
df = pd.read_csv(url)
```
We call the format method, passing positional arguments of bucket - gid-requests, and key - 2019/potholes.csv. We can now pass this URL to something like Pandas with no problems.

**How access is decided**

<img src="/assets/images/20210505_AWSBoto/pic16.png" class="largepic"/>

When a request comes in if it's a presigned URL, it will allow the download. If it's not pre-signed, it will check the policies to make sure they allow the download. AWS's default behavior is to deny access.

**Practice**

**Uploading a public report**
```python
# Upload the final_report.csv to gid-staging bucket
s3.upload_file(
  # Complete the filename
  Filename='./final_report.csv', 
  # Set the key and bucket
  Key='2019/final_report_2019_02_20.csv', 
  Bucket='gid-staging',
  # During upload, set ACL to public-read
  ExtraArgs = {
    'ACL': 'public-read'})
```

**Making multiple files public**
```python
# List only objects that start with '2019/final_'
response = s3.list_objects(
    Bucket='gid-staging', Prefix='2019/final_')

# Iterate over the objects
for obj in response['Contents']:
  
    # Give each object ACL of public-read
    s3.put_object_acl(Bucket='gid-staging', 
                      Key=obj['Key'], 
                      ACL='public-read')
    
    # Print the Public Object URL for each object
    print("https://{}.s3.amazonaws.com/{}".format('gid-staging', obj['Key']))
```

## 2.2. Accessing private objects in S3

If we are **forbidden** to download a private file using pandas:
```python
df = pd.read_csv('https://gid-staging.s3.amazonaws.com/potholes.csv')
```
We can use boto3 `download_file` method to download it and read it from the disk 
```python
s3.download_file( 
    Filename='potholes_local.csv', 
    Bucket='gid-staging', 
    Key='2019/potholes_private.csv')

pd.read_csv('./potholes_local.csv')
```

This is a great option if we expect this file not to change much. We have better way.
**Accessing private files**
**Use `'get_object'`**
```python
obj = s3.get_object(Bucket='gid-requests', Key='2019/potholes.csv') 
print(obj)
```
<img src="/assets/images/20210505_AWSBoto/pic17.png" class="largepic"/>

* We get a Body key, with a "StreamingBody" object as the value. A StreamingBody is a special type of response that doesn't download the whole object immediately.
* Pandas knows how to handle this type of response. We pass the contents of this key to pandas, and it will read it like a CSV file.

Read `StreamingBody` into Pandas
```python
pd.read_csv(obj['Body'])
```

**Pre-signed URLs**
* Expire after a certain timeframe 
* Great for temporary access

**Generate Presigned URL**
```python
share_url = s3.generate_presigned_url( 
    ClientMethod='get_object', 
    ExpiresIn=3600,
    Params={'Bucket': 'gid-requests','Key': 'potholes.csv'}
)

# Open in Pandas
pd.read_csv(share_url)
```
We generate a pre-signed URL that grants access to the file for 1 hour or 3600 seconds. This way, our colleague can open it in pandas or a browser. After 1 hour passes, the access expires.

**Load multiple files into one DataFrame**
```python
# Create list to hold our DataFrames 
df_list = []

# Request the list of csv's from S3 with prefix; Get contents 
response = s3.list_objects(
    Bucket='gid-requests', 
    Prefix='2019/')

# Get response contents 
request_files = response['Contents']

# Iterate over each object 
for file in request_files:
    obj = s3.get_object(Bucket='gid-requests', Key=file['Key'])

    # Read it as DataFrame
    obj_df = pd.read_csv(obj['Body'])

    # Append DataFrame to list 
    df_list.append(obj_df)

# Concatenate all the DataFrames in the list
df = pd.concat(df_list)

# Preview the DataFrame
df.head()
```

<img src="/assets/images/20210505_AWSBoto/pic18.png" class="largepic"/>

**Practice: Opening a private file**
```python
df_list =  [ ] 

for file in response['Contents']:
    # For each file in response load the object from S3
    obj = s3.get_object(Bucket='gid-requests', Key=file['Key'])
    # Load the object's StreamingBody with pandas
    obj_df = pd.read_csv(obj['Body'])
    # Append the resulting DataFrame to list
    df_list.append(obj_df)

# Concat all the DataFrames with pandas
df = pd.concat(df_list)

# Preview the resulting DataFrame
df.head()
```

## 2.3. Sharing files through a website

**Serving HTML**
Many python data science tools have the ability to generate simple HTML from data. The goal is not to have us making beautiful websites, but to share our analyses with the world.

**Convert DataFrame to html**
```python
df.to_html('table_agg.html',render_link=True)
```

<img src="/assets/images/20210505_AWSBoto/pic19.png" class="largepic"/>

The optional `render_link` argument converts any URLs to be clickable.

**Column**

We can use the columns parameter to pass a list of only columns we want rendered. In this example, we are only showing service_name and info_link columns.

<img src="/assets/images/20210505_AWSBoto/pic20.png" class="largepic"/>

**Border**
```python
df.to_html('table_agg.html',
            render_links=True,
            columns['service_name', 'request_count', 'info_link'], 
            border=0)

```
<img src="/assets/images/20210505_AWSBoto/pic21.png" class="largepic"/>

**Uploading HTML file to S3**

```python
s3.upload_file( 
    Filename='./table_agg.html', 
    Bucket='datacamp-website', 
    Key='table.html',
    ExtraArgs = {
        'ContentType': 'text/html', 
        'ACL': 'public-read'}
)
```
Accessing HTML file
```
https://{bucket}.s3.amazonaws.com/{key}
https://datacamp-website.s3.amazonaws.com/table.html
```
<img src="/assets/images/20210505_AWSBoto/pic22.png" class="largepic"/>

**Uploading other types of contents** 
```python
s3.upload_file( 
    Filename='./plot_image.png', 
    Bucket='datacamp-website', 
    Key='plot_image.png', 
    ExtraArgs = {
        'ContentType': 'image/png', 
        'ACL': 'public-read'}
)
```

**IANA Media Types**
* **JSON: `application/json`**
* **PNG: `image/png`**
* **PDF: `application/pdf`**
* **CSV: `text/csv`**

**Generating an index page**

We have multiple HTML and image files that we want to share, and we want to generate a listing for them

```python
# List the gid-reports bucket objects starting with 2019/
r = s3.list_objects(Bucket='gid-reports', Prefix='2019/')

# Convert the response contents to DataFrame
objects_df = pd.DataFrame(r['Contents'])

# Create a column "Link" that contains website url + key 
base_url = "http://datacamp-website.s3.amazonaws.com/" 
objects_df['Link'] = base_url + objects_df['Key']

# Write DataFrame to html 
objects_df.to_html('report_listing.html',
                    columns=['Link', 'LastModified', 'Size'], 
                    render_links=True)
```
<img src="/assets/images/20210505_AWSBoto/pic23.png" class="largepic"/>

**Uploading an index page**
```python
s3.upload_file( 
    Filename='./report_listing.html', 
    Bucket='datacamp-website', 
    Key='index.html',
    ExtraArgs = {
    'ContentType': 'text/html', 
    'ACL': 'public-read'}
)
```
**Practice**
```python
# Generate an HTML table with no border and selected columns
services_df.to_html('./services_no_border.html',
           # Keep specific columns only
           columns=['service_name', 'link'],
           # Set border
           border=0)

# Generate an html table with border and all columns.
services_df.to_html('./services_border_all_columns.html', 
           border=1)
# Upload the lines.html file to S3
s3.upload_file(Filename='lines.html', 
               # Set the bucket name
               Bucket='datacamp-public', 
               Key='index.html',
               # Configure uploaded file
               ExtraArgs = {
                 # Set proper content type
                 'ContentType':'text/html',
                 # Set proper ACL
                 'ACL': 'public-read'})

# Print the S3 Public Object URL for the new file.
print("http://{}.s3.amazonaws.com/{}".format('datacamp-public', 'index.html'))
```


## 2.4. Case Study: Generating a Report Repository

**The steps:**
* Download files for the month from the raw data bucket 
* Concatenate them into one csv
* Create an aggregated DataFrame
* Write the DataFrame to CSV and HTML 
* Generate a Bokeh plot, save as HTML
* Create `grid-reports` bucket
* Upload all the three files for the month to S3 
* Generate an index.html le that lists all the files 
* Get the website URL!

**Raw data bucket**
<img src="/assets/images/20210505_AWSBoto/pic24.png" class="largepic"/>
* Private files
* Daily CSVs of requests from the App 
* Raw Data

**Read raw data files**
```python
# Create list to hold our DataFrames 
df_list = []

# Request the list of csv's from S3 with prefix; Get contents 
response = s3.list_objects(
    Bucket='gid-requests', 
    Prefix='2019_jan')

# Get response contents 
request_files = response['Contents']


# Iterate over each object 
for file in request_files:
    obj = s3.get_object(Bucket='gid-requests', Key=file['Key'])

    # Read it as DataFrame
    obj_df = pd.read_csv(obj['Body'])

    # Append DataFrame to list 
    df_list.append(obj_df

# Concatenate all the DataFrames in the list
df = pd.concat(df_list)

# Preview the DataFrame
df.head()
```
<img src="/assets/images/20210505_AWSBoto/pic25.png" class="largepic"/>

**Create aggregated reports**
* Perform some aggregation
* `df.to_csv('jan_final_report.csv')`
* `df.to_html('jan_final_report.html')`
* `jan_final_chart.html`

**Report bucket**
* Bucket website 
* Publicly Accessible
* Aggregated data and HTML reports

**Upload Aggregated CSV**
```python
# Upload Aggregated CSV to S3 
s3.upload_file(Filename='./jan_final_report.csv',
               Key='2019/jan/final_report.csv', 
               Bucket='gid-reports',
               ExtraArgs = {'ACL': 'public-read'})
```

**Create index.html**
```python
# List the gid-reports bucket objects starting with 2019/ 
r = s3.list_objects(Bucket='gid-reports', Prefix='2019/')

# Convert the response contents to DataFrame 
objects_df = pd.DataFrame(r['Contents'])

# Create a column "Link" that contains website url + key 
base_url = "https://gid-reports.s3.amazonaws.com/" 
objects_df['Link'] = base_url + objects_df['Key']

# Write DataFrame to html 
objects_df.to_html('report_listing.html',
                   columns=['Link', 'LastModified', 'Size'], 
                   render_links=True)

```

**Upload index.html**
```python
# Upload the file to gid-reports bucket root. 
s3.upload_file(
    Filename='./report_listing.html', 
    Key='index.html',
    Bucket='gid-reports', 
    ExtraArgs = {
    'ContentType': 'text/html', 
    'ACL': 'public-read'
})
```
**Get the URL of the index!**

```
"http://gid-reports.s3.amazonaws.com/index.html"
```
**Practice**
**Combine daily requests for February**

```python
df_list = [] 

# Load each object from S3
for file in request_files:
    s3_day_reqs = s3.get_object(Bucket='gid-requests', 
                                Key=file['Key'])
    # Read the DataFrame into pandas, append it to the list
    day_reqs = pd.read_csv(s3_day_reqs['Body'])
    df_list.append(day_reqs)

# Concatenate all the DataFrames in the list
all_reqs = pd.concat(df_list)

# Preview the DataFrame
all_reqs.head()
```
**Upload aggregated reports for February**
```python
# Write agg_df to a CSV and HTML file with no border
agg_df.to_csv('./feb_final_report.csv')
agg_df.to_html('./feb_final_report.html', border=0)

# Upload the generated CSV to the gid-reports bucket
s3.upload_file(Filename='./feb_final_report.csv', 
	Key='2019/feb/final_report.html', Bucket='gid-reports',
    ExtraArgs = {'ACL': 'public-read'})

# Upload the generated HTML to the gid-reports bucket
s3.upload_file(Filename='./feb_final_report.html', 
	Key='2019/feb/final_report.html', Bucket='gid-reports',
    ExtraArgs = {'ContentType': 'text/html', 
                 'ACL': 'public-read'})
```
**Update index to include February**
```python
# List the gid-reports bucket objects starting with 2019/
objects_list = s3.list_objects(Bucket='gid-reports', Prefix='2019/')

# Convert the response contents to DataFrame
objects_df = pd.DataFrame(objects_list['Contents'])

# Create a column "Link" that contains Public Object URL
base_url = "http://gid-reports.s3.amazonaws.com/"
objects_df['Link'] = base_url + objects_df['Key']

# Preview the resulting DataFrame
objects_df.head()
```
**Upload the new index**
```python
# Write objects_df to an HTML file
objects_df.to_html('report_listing.html',
    # Set clickable links
    render_links=True,
	# Isolate the columns
    columns=['Link', 'LastModified', 'Size'])

# Overwrite index.html key by uploading the new file
s3.upload_file(
  Filename='./report_listing.html', Key='index.html', 
  Bucket='gid-reports',
  ExtraArgs = {
    'ContentType': 'text/html', 
    'ACL': 'public-read'
  })
```

# 3. Reporting and Notifying

Learn how to automate sharing your findings with the world by building notification triggers for your analysis, how to harness AWS to send SMS and email notifications to users. Cap off by making custom notifications depending on a user's needs.

## 3.1. SNS Topics

<img src="/assets/images/20210505_AWSBoto/pic26.png" class="largepic"/>

Amazon SNS stands for **simple notification service**. With it, we can send emails, text messages and push notifications. **Publishers** post messages to topics and **subscribers** receive them.In SNS, we publish messages to an SNS **topic**, and subscribers receive those messages via email or text.

**Creating an SNS topic**
```python
sns = boto3.client('sns',
                    region_name='us-east-1', 
                    aws_access_key_id=AWS_KEY_ID, 
                    aws_secret_access_key=AWS_SECRET)

# Create topic
response = sns.create_topic(Name='city_alerts')
```

In return, we get an API response from AWS. **ARN**: unique ID for the topic
<img src="/assets/images/20210505_AWSBoto/pic27.png" class="largepic"/>


```python
topic_arn = response['TopicArn']

# or shortcut
sns.create_topic(Name='city_alerts')['TopicArn']
```

**Listing topic**
```python
response = sns.list_topics()
```

<img src="/assets/images/20210505_AWSBoto/pic28.png" class="largepic"/>

**Deleting topics**
```python
sns.delete_topic(TopicArn='arn:aws:sns:us-east-1:320333787981:city_alerts')
```

**Creating multiple topics**
```python
# Create list of departments
departments = ['trash', 'streets', 'water']

for dept in departments:
  	# For every department, create a general topic (ex: 'streets_general')
    sns.create_topic(Name="{}_general".format(dept))
    
    # For every department, create a critical topic (ex: 'streets_critical')
    sns.create_topic(Name="{}_critical".format(dept))

# Print all the topics in SNS
response = sns.list_topics()
print(response['Topics'])
```

**Deleting multiple topics**

```python
# Get the current list of topics
topics = sns.list_topics()['Topics']

for topic in topics:
  # For each topic, if it is not marked critical, delete it
  if "critical" not in topic['TopicArn']:
    sns.delete_topic(TopicArn=topic['TopicArn'])
    
# Print the list of remaining critical topics
print(sns.list_topics()['Topics'])
```

## 3.2. SNS Subscriptions

**Creating an SMS Subscriptions**
```python
sns = boto3.client('sns',
                    region_name='us-east-1', 
                    aws_access_key_id=AWS_KEY_ID, 
                    aws_secret_access_key=AWS_SECRET)

response = sns.subscribe(
    TopicArn = 'arn:aws:sns:us-east-1:320333787981:city_alerts',
    Protocol = 'SMS',
    Endpoint = '+13125551123')
```

In our response, we get a dictionary that contains a 'SubscriptionArn' key. 

<img src="/assets/images/20210505_AWSBoto/pic29.png" class="largepic"/>


**Creating an email Subscriptions**
```python
response = sns.subscribe(
        TopicArn = 'arn:aws:sns:us-east-1:320333787981:city_alerts', 
        Protocol='email',
        Endpoint='max@maksimize.com')
```

<img src="/assets/images/20210505_AWSBoto/pic29.png" class="largepic"/>

The status can either be confirmed or pending confirmation. Phone numbers are automatically confirmed, but for email, the user has to click a unique link to authorize the subscription.

**Listing subscriptions by Topic**
```python
sns.list_subscriptions_by_topic(
    opicArn='arn:aws:sns:us-east-1:320333787981:city_alerts')
```
<img src="/assets/images/20210505_AWSBoto/pic31.png" class="largepic"/>

**Listing subscriptions for all Topics**
```python
sns.list_subscriptions()['Subscriptions']
```

**Deleting subscriptions**
```python
sns.unsubscribe(
    SubscriptionArn='arn:aws:sns:us-east-1:320333787981:city_alerts:9f2dad1d-8844-4fe8
)
```

**Deleteting multiple subscriptions**
```python
# Get list of subscriptions

response = sns.list_subscriptions_by_topic( 
    TopicArn='arn:aws:sns:us-east-1:320333787981:city_alerts')

subs = response['Subscriptions']

# Unsubscribe SMS subscriptions
for sub in subs:
    if sub['Protocol'] == 'sms': 
        sns.unsubscribe(sub['SubscriptionArn'])
```

**Creating multiple Subscriptions**
```python
# For each email in contacts, create subscription to street_critical
for email in contacts['Email']:
  sns.subscribe(TopicArn = str_critical_arn,
                # Set channel and recipient
                Protocol = 'email',
                Endpoint = email)

# List subscriptions for streets_critical topic, convert to DataFrame
response = sns.list_subscriptions_by_topic(
  TopicArn = str_critical_arn)
subs = pd.DataFrame(response['Subscriptions'])

# Preview the DataFrame
subs.head()
```
**Deleting multiple Subscriptions**
```python
# List subscriptions for streets_critical topic.
response = sns.list_subscriptions_by_topic(
  TopicArn = str_critical_arn)

# For each subscription, if the protocol is SMS, unsubscribe
for sub in response['Subscriptions']:
  if sub['Protocol'] == 'sms':
	  sns.unsubscribe(SubscriptionArn=sub['SubscriptionArn'])

# List subscriptions for streets_critical topic in one line
subs = sns.list_subscriptions_by_topic(
  TopicArn=str_critical_arn)['Subscriptions']

# Print the subscriptions
print(subs)
```

## 3.3. Sending messages

**Publishing a topic**
```python
response = sns.publish(
    TopicArn = 'arn:aws:sns:us-east-1:320333787981:city_alerts', 
    Message = 'Body text of SMS or e-mail',
    Subject = 'Subject Line for Email'
)
```
**Sending a custom messages**
```python
num_of_reports = 137

response = client.publish(
    TopicArn = 'arn:aws:sns:us-east-1:320333787981:city_alerts',
    Message = 'There are {} reports outstanding'.format(num_of_reports), 
    Subject = 'Subject Line for Email'
)
```

**Sending a single SMS**
```python
response = sns.publish( 
    PhoneNumber = '+13121233211',
    Message = 'Body text of SMS or e-mail'
)
```
If we use this method, we don't have to worry about having a topic or subscribers to it. We simply send a text message to a phone number.

**Publishing to Topic vs Single SMS**
* Publising to Topic:
    * Have to have a topic
    * Our topic has to have subscriptions
    * Better for multiple receivers
    * Easier list management
    * Topic and subscribers = maintenability
* Send a single SMS:
    * Don't need a topic
    * Don't need subscriptions
    * Just sends a message to a phone number
    * Email option not available
    * One-off texts = getting stuff done

**Sending an alert**
```python
# If there are over 100 potholes, create a message
if streets_v_count > 100:
  # The message should contain the number of potholes.
  message = "There are {} potholes!".format(streets_v_count)
  # The email subject should also contain number of potholes
  subject = "Latest pothole count is {}".format(streets_v_count)

  # Publish the email to the streets_critical topic
  sns.publish(
    TopicArn = str_critical_arn,
    # Set subject and message
    Message = message,
    Subject = subject
  )
```

**Sending a single SMS message**
```python
# Loop through every row in contacts
for idx, row in contacts.iterrows():
    
    # Publish an ad-hoc sms to the user's phone number
    response = sns.publish(
        # Set the phone number
        PhoneNumber = str(row['Phone']),
        # The message should include the user's name
        Message = 'Hello {}'.format(row['Name'])
    )
   
    print(response)
```

## 3.4. Case Study: Building a notification system

<img src="/assets/images/20210505_AWSBoto/pic32.png" class="largepic"/>

**Building a notification**
* **Topic set up**
    * Create the topic
    * Download the contact list csv
    * Create topics for each service
    * Subscribe the contacts to their respective topics
* **Get the aggregated numbers**
    * Download the monthly get it done report
    * Get the count of Potholes
    * Get the count of Illegal dumping notifications
* **Send Alerts**
    * If potholes exceeds 100, send alert
    * If illegal dumping exceeds 30, send alert

**Topic set up**
```python
# Initialize SNS client
sns = boto3.client('sns',
    region_name='us-east-1', 
    aws_access_key_id=AWS_KEY_ID, 
    aws_secret_access_key=AWS_SECRET)

# Create topics and store their ARNs
trash_arn = sns.create_topic(Name="trash_notifications")['TopicArn'] 
streets_arn = sns.create_topic(Name="streets_notifications")['TopicArn']
```

**Subscribing users to topics**
```python
# Create subscribe_user method
contacts = pd.read_csv('http://gid-staging.s3.amazonaws.com/contacts.csv')
def subscribe_user(user_row):
    if user_row['Department'] == 'trash':
        sns.subscribe(TopicArn = trash_arn, Protocol='sms', Endpoint=str(user_row['Phone'])) 
        sns.subscribe(TopicArn = trash_arn, Protocol='email', Endpoint=user_row['Email'])
    else:
        sns.subscribe(TopicArn = streets_arn, Protocol='sms', Endpoint=str(user_row['Phone'])) 
        sns.subscribe(TopicArn = streets_arn, Protocol='email', Endpoint=user_row['Email'])

# Apply the subscribe_user method to every row
contacts.apply(subscribe_user, axis=1)
```
**Get the aggregated numbers**
```python
# Load January's report into a DataFrame
df = pd.read_csv('http://gid-reports.s3.amazonaws.com/2019/feb/final_report.csv')

# Set the index so we can access counts by service name directly
df.set_index('service_name', inplace=True)

# Get the aggregated numbers
trash_violations_count = df.at['Illegal Dumping', 'count'] 
streets_violations_count = df.at['Pothole', 'count']
```

**Send Alerts**
```python
if trash_violations_count > 100: 
    # Construct the message to send
    message = "Trash violations count is now {}".format(trash_violations_count) 
    # Send message
    sns.publish(TopicArn = trash_arn,
                Message = message, 
                Subject = "Trash Alert")


if streets_violations_count > 30:
    # Construct the message to send
    message = "Streets violations count is now {}".format(streets_violations_count) 
    # Send message
    sns.publish(TopicArn = streets_arn,
                Message = message, Subject = "Streets Alert")

```

**Creating multi-level topics**
```python
dept_arns = {}

for dept in departments:
  # For each deparment, create a critical Topic
  critical = sns.create_topic(Name="{}_critical".format(dept))
  # For each department, create an extreme Topic
  extreme = sns.create_topic(Name="{}_extreme".format(dept))
  # Place the created TopicARNs into a dictionary 
  dept_arns['{}_critical'.format(dept)] = critical['TopicArn']
  dept_arns['{}_extreme'.format(dept)] = extreme['TopicArn']

# Print the filled dictionary.
print(dept_arns)
```

**Different protocols per topic level**
```python
for index, user_row in contacts.iterrows():
  # Get topic names for the users's dept
  critical_tname = "{}_critical".format(user_row['Department'])
  extreme_tname = "{}_extreme".format(user_row['Department'])
  
  # Get or create the TopicArns for a user's department.
  critical_arn = sns.create_topic(Name=critical_tname)['TopicArn']
  extreme_arn = sns.create_topic(Name=extreme_tname)['TopicArn']
  
  # Subscribe each users email to the critical Topic
  sns.subscribe(TopicArn = critical_arn, 
                Protocol='email', Endpoint=user_row['Email'])
  # Subscribe each users phone number for the extreme Topic
  sns.subscribe(TopicArn = extreme_arn, 
                Protocol='sms', Endpoint=str(user_row['Phone']))
```

**Sending multi-level alerts**
```python
if vcounts['water'] > 100:
  # If over 100 water violations, publish to water_critical
  sns.publish(
    TopicArn = dept_arns['water_critical'],
    Message = "{} water issues".format(vcounts['water']),
    Subject = "Help fix water violations NOW!")

if vcounts['water'] > 300:
  # If over 300 violations, publish to water_extreme
  sns.publish(
    TopicArn = dept_arns['water_extreme'],
    Message = "{} violations! RUN!".format(vcounts['water']),
    Subject = "THIS IS BAD.  WE ARE FLOODING!")
```

# 4. Rekognizing patterns

Go beyond uploading, sharing and notifying into rekognizing using AWS Rekognition and other AWS machine learning services to recognize cats, translate language and detect sentiment.

## 4.1. Rekognizing patterns

**Upload an image to S3**
```python
s3 = boto3.client(
    's3', region_name='us-east-1',
    aws_access_key_id=AWS_KEY_ID, 
    aws_secret_access_key=AWS_SECRET
)

s3.upload_file(
    Filename='report.jpg', Key='report.jpg', 
    Bucket='datacamp-img')

```

**Object detection**
**Initiate the client**

```python
rekog = boto3.client( 
    'rekognition', region_name='us-east-1', 
    aws_access_key_id=AWS_KEY_ID,
    aws_secret_access_key=AWS_SECRET)
```

**Detect**
```python
    response = rekog.detect_labels( 
        Image={'S3Object': {
               'Bucket': 'datacamp-img', 
               'Name': 'report.jpg'
                },
        MaxLabels=10, 
        MinConfidence=95
)
```
Call detect_labels, specifying Bucket and Image to get the response from Rekognition. Optionally, we can specify the maximum amount of labels to return and the minimum confidence in the match that we are willing to accept.

<img src="/assets/images/20210505_AWSBoto/pic33.png" class="largepic"/>

The response will come back as a list of labels. This label Name is Bicycle. We are 99% Confident that there is a Bicycle in this image.

**Text detection**
```python
response = rekog.detect_text( 
    Image={'S3Object':
            {
                'Bucket': 'datacamp-img', 
                'Name': 'report.jpg'
            }
        }
)
```

<img src="/assets/images/20210505_AWSBoto/pic34.png" class="largepic"/>


**Cat detector**
```python
# Use Rekognition client to detect labels
image1_response = rekog.detect_labels(
    # Specify the image as an S3Object; Return one label
    Image=image1, MaxLabels=1)

# Print the labels
print(image1_response['Labels'])

# Use Rekognition client to detect labels
image2_response = rekog.detect_labels(
    # Specify the image as an S3Object; Return one label
    Image=image2, MaxLabels=1)

# Print the labels
print(image2_response['Labels'])

[{'Confidence': 99.85968017578125, 'Instances': [], 'Name': 'Walkway', 'Parents': [{'Name': 'Path'}]}]
[{'Confidence': 96.95977020263672, 'Instances': [{'BoundingBox': {'Height': 0.3252439796924591, 'Left': 0.668968915939331, 'Top': 0.14526571333408356, 'Width': 0.1563364714384079}, 'Confidence': 96.95977020263672}], 'Name': 'Cat', 'Parents': [{'Name': 'Pet'}, {'Name': 'Mammal'}, {'Name': 'Animal'}]}]
```

**Multiple cat detector**

```python
# Create an empty counter variable
cats_count = 0
# Iterate over the labels in the response
for label in response['Labels']:
    # Find the cat label, look over the detected instances
    if label['Name'] == 'Cat':
        for instance in label['Instances']:
            # Only count instances with confidence > 85
            if (instance['Confidence'] > 85):
                cats_count += 1
# Print count of cats
print(cats_count)
```

**Parking sign reader**
```python
# Create empty list of words
words = []
# Iterate over the TextDetections in the response dictionary
for text_detection in response['TextDetections']:
  	# If TextDetection type is WORD, append it to words list
    if text_detection['Type'] == 'WORD':
        # Append the detected text
        words.append(text_detection['DetectedText'])
# Print out the words list
print(words)

# Create empty list of lines
lines = []
# Iterate over the TextDetections in the response dictionary
for text_detection in response['TextDetections']:
  	# If TextDetection type is Line, append it to lines list
    if text_detection['Type'] == 'LINE':
        # Append the detected text
        lines.append(text_detection['DetectedText'])
# Print out the words list
print(lines)
```

## 4.2. Comprehending text

**Translating Text**
```python
# Initialize client
translate = boto3.client('translate', 
    region_name='us-east-1',
    aws_access_key_id=AWS_KEY_ID, aws_secret_access_key=AWS_SECRET)

# Translate text
response = translate.translate_text( 
    Text='Hello, how are you?', 
    SourceLanguageCode='auto', 
    TargetLanguageCode='es')
```

<img src="/assets/images/20210505_AWSBoto/pic35.png" class="largepic"/>

**Get translate text directly**
```python
translated_text = translate.translate_text( 
    Text='Hello, how are you?', 
    SourceLanguageCode='auto', 
    TargetLanguageCode='es')['TranslatedText']
```

**Detecting language: AWS Comprehend**
```python
# Initialize boto3 Comprehend client
comprehend = boto3.client('comprehend', 
                          region_name='us-east-1',
                          aws_access_key_id=AWS_KEY_ID, 
                          aws_secret_access_key=AWS_SECRET)

# Detect dominant language
response = comprehend.detect_dominant_language(
    Text="Hay basura por todas partes a lo largo de la carretera.")

```
<img src="/assets/images/20210505_AWSBoto/pic36.png" class="largepic"/>

**Understanding sentiment**
**Detect text sentiment**
```python
response = comprehend.detect_sentiment( 
    Text="DataCamp students are amazing.", 
    LanguageCode='en')

# Direct response
sentiment = comprehend.detect_sentiment( 
    Text='Maksim is amazing.', 
    LanguageCode='en')['Sentiment']
```

**Detecting language**
```python
# For each dataframe row
for index, row in dumping_df.iterrows():
    # Get the public description field
    description = dumping_df.loc[index, 'public_description']
    if description != '':
        # Detect language in the field content
        resp = comprehend.detect_dominant_language(Text=description)
        # Assign the top choice language to the lang column.
        dumping_df.loc[index, 'lang'] = resp['Languages'][0]['LanguageCode']
        
# Count the total number of spanish posts
spanish_post_ct = len(dumping_df[dumping_df.lang == 'es'])
# Print the result
print("{} posts in Spanish".format(spanish_post_ct))
```

**Translating Get It Done requests**
```python
for index, row in dumping_df.iterrows():
  	# Get the public_description into a variable
    description = dumping_df.loc[index, 'public_description']
    if description != '':
      	# Translate the public description
        resp = translate.translate_text(
            Text=description, 
            SourceLanguageCode='auto', TargetLanguageCode='en')
        # Store original language in original_lang column
        dumping_df.loc[index, 'original_lang'] = resp['SourceLanguageCode']
        # Store the translation in the translated_desc column
        dumping_df.loc[index, 'translated_desc'] = resp['TranslatedText']
# Preview the resulting DataFrame
dumping_df = dumping_df[['service_request_id', 'original_lang', 'translated_desc']]
dumping_df.head()
```
**Getting request sentiment**
```python
for index, row in dumping_df.iterrows():
  	# Get the translated_desc into a variable
    description = dumping_df.loc[index, 'public_description']
    if description != '':
      	# Get the detect_sentiment response
        response = comprehend.detect_sentiment(
          Text=description, 
          LanguageCode='en')
        # Get the sentiment key value into sentiment column
        dumping_df.loc[index, 'sentiment'] = response['Sentiment']
# Preview the dataframe
dumping_df.head()
```

## 4.3. Case Study: Scooting Around!
**The final product**
```python
rekog = boto3.client('rekognition',
                     region_name='us-east-1', 
                     aws_access_key_id=AWS_KEY_ID, 
                     aws_secret_access_key=AWS_SECRET)

comprehend = boto3.client('comprehend',
                          region_name='us-east-1', 
                          aws_access_key_id=AWS_KEY_ID, 
                          aws_secret_access_key=AWS_SECRET)

translate = boto3.client('translate',
                         region_name='us-east-1', 
                         aws_access_key_id=AWS_KEY_ID, 
                         aws_secret_access_key=AWS_SECRET)

# Translate all descriptions to English

for index, row in df.iterrows():
    desc = df.loc[index, 'public_description'] 
    if desc != '':
        resp = translate_fake.translate_text( 
            Text=desc, 
            SourceLanguageCode='auto', 
            TargetLanguageCode='en')
        df.loc[index, 'public_description'] = resp['TranslatedText']


# Detect text sentiment

for index, row in df.iterrows():
    desc = df.loc[index, 'public_description'] 
        if desc != '':
            resp = comprehend.detect_sentiment( 
                Text=desc,
                LanguageCode='en')
            df.loc[index, 'sentiment'] = resp['Sentiment']

# Detect scooter in image
df['img_scooter'] = 0
for index, row in df.iterrows(): 
    image = df.loc[index, 'image'] 
    response = rekog.detect_labels(
        # Specify the image as an S3Object
            Image={'S3Object': {'Bucket': 'gid-images', 'Name': image}}

    )

    for label in response['Labels']: 
        if label['Name'] == 'Scooter':
        df.loc[index, 'img_scooter'] = 1 
        reak
# Final count!
# Select only rows where there was a scooter image and that have negative sentiment

pickups = df[((df.img_scooter == 1) & (df.sentiment == 'NEGATIVE'))]
num_pickups = len(pickups)

332 Scooters!
```
**Practice**

**Scooter community sentiment**
* For Every DataFrame row, detect the dominant language.
* Use the detected language to determine the sentiment of the description.
* Group the DataFrame by the 'sentiment' and 'lang' columns in that order.

```python
# Scooter community sentiment
for index, row in scooter_requests.iterrows():
  	# For every DataFrame row
    desc = scooter_requests.loc[index, 'public_description']
    if desc != '':
      	# Detect the dominant language
        resp = comprehend.detect_dominant_language(Text=desc)
        lang_code = resp['Languages'][0]['LanguageCode']
        scooter_requests.loc[index, 'lang'] = lang_code
        # Use the detected language to determine sentiment
        scooter_requests.loc[index, 'sentiment'] = comprehend.detect_sentiment(
          Text=desc, 
          LanguageCode=lang_code)['Sentiment']
# Perform a count of sentiment by group.
counts = scooter_requests.groupby(['sentiment', 'lang']).count()
counts.head()
```

**Scooter dispatch**
* Get the SNS topic ARN for `'scooter_notifications'`.
* For every row, if sentiment is `'NEGATIVE'` and there is an image of a scooter, construct a message to send.
* Publish the notification to the SNS topic.

```python
# Get topic ARN for scooter notifications
topic_arn = sns.create_topic(Name='scooter_notifications')['TopicArn']

for index, row in scooter_requests.iterrows():
    # Check if notification should be sent
    if (row['sentiment'] == 'NEGATIVE') & (row['img_scooter'] == 1):
        # Construct a message to publish to the scooter team.
        message = "Please remove scooter at {}, {}. Description: {}".format(
            row['long'], row['lat'], row['public_description'])

        # Publish the message to the topic!
        sns.publish(TopicArn = topic_arn,
                    Message = message, 
                    Subject = "Scooter Alert")
```
# 5. Reference

1. [Introduction to AWS Boto in Python- DataCamp](https://learn.datacamp.com/courses/introduction-to-aws-boto-in-python)