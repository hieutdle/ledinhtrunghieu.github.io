---
layout: post
author: ledinhtrunghieu
title: Lesson 14 - Introduction to AWS Boto in Python
---

# 1. Putting Files in the Cloud

Learning how AWS works to creating S3 buckets and uploading files to them. 

## 1.1. Intro to AWS and Boto3

**What is Boto3**

To interact with AWS in Python, we use the Boto3 library. 

```python
import 

s3 = boto3.client('s3',
                  region_name='us-east-1', 
                  aws_access_key_id=AWS_KEY_ID, 
                  aws_secret_access_key=AWS_SECRET)

response = s3.list_buckets()


```

We initialize the client with an AWS service name, region, key and secret. The service name can be any of the 100+ available AWS services. The region is the geo region where our resources are located. **Key** and **secret** are like a username and password to an AWS account.

**Creating keys with IAM**

<img src="/assets/images/20210505_AWSBoto/pic1.png" class="largepic"/>

* Creating an account at aws.amazon.com gives us access to the AWS Console. Anything we do here to manage our web services, we can do in Boto3 in Python.
* To log into the console, we use the username/password we signed up with. This is the root user.
* To create the key/secret for Boto3, we are going to use **IAM** or **Identity Access Management Service**. We create IAM sub-users to control access to AWS resources in our account. Credentials - or the key / secret combo are what authenticate IAM users.

**AWS services**
* **S3** or **Simple Storage Service** lets us store files in the cloud.
* **SNS** or **Simple Notification Service** lets us send emails and texts to alert subscribers based on events and conditions in our data pipelines.
* **Comprehend** performs sentiment analysis on blocks of text.
* **Rekognition** to extracts text from images and look for cats in a picture
* **RDS**, **EC2**, **Lambda** and other common services.


**Multiple clients**
```python
# Generate the boto3 client for interacting with S3 and SNS
s3 = boto3.client('s3', region_name='us-east-1', 
                         aws_access_key_id=AWS_KEY_ID, 
                         aws_secret_access_key=AWS_SECRET)

sns = boto3.client('sns', region_name='us-east-1', 
                         aws_access_key_id=AWS_KEY_ID, 
                         aws_secret_access_key=AWS_SECRET)

# List S3 buckets and SNS topics
buckets = s3.list_buckets()
topics = sns.list_topics()

# Print out the list of SNS topics
print(topics)
```

## 1.2. Diving into buckets

**S3** lets us put any file in the cloud and make it accessible anywhere in the world through a URL

**S3 Components - Buckets**
* **Buckets** are like folders on our desktop. **Objects** are like files within those folders. 
* Buckets have their own permissions policies. 
* Can be configured to act as folders for a static website
* Can generate **logs** about their own activity and write them to a different bucket.
* **Contain Object**


**S3 Components - Objects**
<img src="/assets/images/20210505_AWSBoto/pic2.png" class="largepic"/>
An object can be anything - an image, a video file, CSV or a log file.

**What can we do with buckets**
* Create Bucket
* List Buckets 
* Delete Bucket

```python
import boto3
s3 = boto3.client('s3', 
                  region_name='us-east-1',
                  aws_access_key_id=AWS_KEY_ID, 
                  aws_secret_access_key=AWS_SECRET)
```

**Creating a Bucket**
```python
bucket = s3.create_bucket(Bucket='gid-requests')
```
We call the client's create_bucket method, passing the bucket name as the argument.


**List Buckets**
```python
bucket_response = s3.list_buckets()
```
When S3 responds, it will give us some additional response metadata. it will include a dictionary under the Buckets key. 

**Get Buckets Dictionary**
```python
buckets = bucket_response['Buckets'] 
print(buckets)
```
<img src="/assets/images/20210505_AWSBoto/pic3.png" class="largepic"/>
We can see our new bucket name and the time that it was created. Now that we have this dictionary, we can run it through a for loop and perform an operation on multiple buckets.

**Deleting Buckets**
```python
response = s3.delete_bucket('gid-requests')
```

**Practice**

```python
import boto3

# Create boto3 client to S3
s3 = boto3.client('s3', region_name='us-east-1', 
                         aws_access_key_id=AWS_KEY_ID, 
                         aws_secret_access_key=AWS_SECRET)

# Create the buckets
response_staging = s3.create_bucket(Bucket='gim-staging')
response_processed = s3.create_bucket(Bucket='gim-processed')
response_test = s3.create_bucket(Bucket='gim-test')

# Print out the response
print(response_staging)

# Get the list_buckets response
response = s3.list_buckets()

# Iterate over Buckets from .list_buckets() response
for bucket in response['Buckets']:
  
  	# Print the Name for each bucket
    print(bucket['Name'])

# Delete the gim-test bucket
s3.delete_bucket(Bucket='gim-test')

# Get the list_buckets response
response = s3.list_buckets()

# Print each Buckets Name
for bucket in response['Buckets']:
    print(bucket['Name'])

# Get the list_buckets response
response = s3.list_buckets()

# Delete all the buckets with 'gim', create replacements.
for bucket in response['Buckets']:
  if 'gim' in bucket['Name']:
      s3.delete_bucket(Bucket=bucket['Name'])
    
s3.create_bucket(Bucket='gid-staging')
s3.create_bucket(Bucket='gid-processed')
  
# Print bucket listing after deletion
response = s3.list_buckets()
for bucket in response['Buckets']:
    print(bucket['Name'])
```

## 1.3. Uploading and retrieving files

**Buckets and Object**

<img src="/assets/images/20210505_AWSBoto/pic4.png" class="largepic"/>

**A Bucket**
<img src="/assets/images/20210505_AWSBoto/pic5.png" class="largepic"/>

* A bucket has a **name**
* **Name** is a string
* **Unique** name in all of S3.
* Contains **many object**.

**An Object**
<img src="/assets/images/20210505_AWSBoto/pic6.png" class="largepic"/>

* An object has a **key**
* **Name** is full path from bucket root 
* **Unique** key in the bucket
* Can only be in **one** parent bucket

**Uploading files**
<img src="/assets/images/20210505_AWSBoto/pic7.png" class="largepic"/>

```python
s3.upload_file( 
    Filename='gid_requests_2019_01_01.csv' 
    Bucket='gid-requests', 
    Key='gid_requests_2019_01_01.csv')
```

* The Filename is the local file path. 
* Bucket parameter takes the name of the bucket we are uploading to. 
* Key is what we want to name the object in S3. 

**Listing objects in a bucket**
<img src="/assets/images/20210505_AWSBoto/pic8.png" class="largepic"/>

```python
response = s3.list_objects( 
    Bucket='gid-requests', 
    MaxKeys=2,
    Prefix='gid_requests_2019_')

print(response)
```

* We can limit the response to two objects with the MaxKeys argument. 
* Another way to limit the response is to use the optional Prefix argument. Passing it will limit the response to objects that start with the string we provide.

<img src="/assets/images/20210505_AWSBoto/pic9.png" class="largepic"/>


**Getting object metadata**

If we want to know these things about a single object, we can use the client's `head_object` method, passing the `bucket name` and `object key`.

```python
response = s3.head_object( 
    Bucket='gid-requests',
    Key='gid_requests_2018_12_30.csv')

print(response)
```

<img src="/assets/images/20210505_AWSBoto/pic10.png" class="largepic"/>
Notice that because we are only working with one object, there is no Contents dictionary. The metadata is directly in the response dictionary.

**Downloading files**
<img src="/assets/images/20210505_AWSBoto/pic11.png" class="largepic"/>

```python
s3.download_file( 
    Filename='gid_requests_downed.csv', 
    Bucket='gid-requests', 
    Key='gid_requests_2018_12_30.csv')
```

**Deleting object**
<img src="/assets/images/20210505_AWSBoto/pic12.png" class="largepic"/>


```python
s3.delete_object( 
    Bucket='gid-requests',
    Key='gid_requests_2018_12_30.csv')
```

**Practice**
```python
# Upload final_report.csv with key 2019/final_report_01_01.csv
s3.upload_file(Bucket='gid-staging', 
               # Set filename and key
               Filename='final_report.csv', 
               Key='2019/final_report_01_01.csv')

# Get object metadata and print it
response = s3.head_object(Bucket='gid-staging', 
                       Key='2019/final_report_01_01.csv')

# Print the size of the uploaded object
print(response['ContentLength'])

# List only objects that start with '2018/final_'
response = s3.list_objects(Bucket='gid-staging', 
                           Prefix='2018/final_')

# Iterate over the objects
if 'Contents' in response:
  for obj in response['Contents']:
      # Delete the object
      s3.delete_object(Bucket='gid-staging', Key=obj['Key'])

# Print the keys of remaining objects in the bucket
response = s3.list_objects(Bucket='gid-staging')

for obj in response['Contents']:
  	print(obj['Key'])
```

# 2. Sharing Files Securely

## 2.1. Keeping objects secure
**Why care about permissions?**
AWS **defaults to denying permission**. That means if we upload a file, by default only our key and secret have access to it.

```python
df = pd.read_csv('https://gid-staging.s3.amazonaws.com/potholes.csv')
```
<img src="/assets/images/20210505_AWSBoto/pic13.png" class="largepic"/>
If we or our colleague tries to download a file with pandas, S3 won't let them. Even if it's in the same script that uploaded it!

If we initialize the s3 client with our credentials, it will work!

```python
# Generate the boto3 client for interacting with S3 
s3 = boto3.client('s3', region_name='us-east-1',
                        aws_access_key_id=AWS_KEY_ID, 
                        aws_secret_access_key=AWS_SECRET)

# Use client to download a file 
s3.download_file(
    Filename='potholes.csv', 
    Bucket='gid-requests', 
    Key='potholes.csv')
```

**AWS Permissions Systems**

<img src="/assets/images/20210505_AWSBoto/pic14.png" class="largepic"/>

There are 4 ways we can control permissions in S3:
* Use **IAM** to control users' access to AWS services, buckets, and objects: give permissions by attaching IAM policies to a user. IAM applies across all AWS services
* **Bucket** policies give us control on the bucket and the objects within it.
* **ACLs** or access control lists let us set permissions on specific objects within a bucket.
* **presigned URLs** let us provide temporary access to an object.

**ACLs**

**2 types of ACL: `private-read` and `public-read`.**

<img src="/assets/images/20210505_AWSBoto/pic15.png" class="largepic"/>


**Upload File**
```python
s3.upload_file(
    Filename='potholes.csv', Bucket='gid-requests', Key='potholes.csv')
```
Say we upload a file. By default, its ACL is 'private'. Let's set the ACL to 'public-read' using s3_put_object_acl method. Now anyone in the world can download this file.

**Set ACL to `public-read`**
```python
s3.put_object_acl(
    Bucket='gid-requests', Key='potholes.csv', ACL='public-read')
```

**Set ACL on upload: Upload file with `public-read` ACL**

```python
s3.upload_file( 
    Bucket='gid-requests', 
    Filename='potholes.csv', 
    Key='potholes.csv',
    ExtraArgs={'ACL':'public-read'})
```
We pass a dictionary with key ACL and value 'public-read' to the ExtraArgs parameter.

**Accessing public objects**
**S3 Object URL Templete**
```
https://{bucket}.s3.amazonaws.com/{key}
```
**URL for Key=`'2019/potheles.csv'`**
```
https://gid-requests.s3.amazonaws.com/2019/potholes.csv
```

**Generating public object URL**

* **Generate Object URL String**
```python
url = "https://{}.s3.amazonaws.com/{}".format( 
    "gid-requests",
    "2019/potholes.csv")
```
* **`'https://gid-requests.s3.amazonaws.com/2019/potholes.csv'`**
```python
# Read the URL into Pandas 
df = pd.read_csv(url)
```
We call the format method, passing positional arguments of bucket - gid-requests, and key - 2019/potholes.csv. We can now pass this URL to something like Pandas with no problems.

**How access is decided**

<img src="/assets/images/20210505_AWSBoto/pic16.png" class="largepic"/>

When a request comes in if it's a presigned URL, it will allow the download. If it's not pre-signed, it will check the policies to make sure they allow the download. AWS's default behavior is to deny access.

**Practice**

**Uploading a public report**
```python
# Upload the final_report.csv to gid-staging bucket
s3.upload_file(
  # Complete the filename
  Filename='./final_report.csv', 
  # Set the key and bucket
  Key='2019/final_report_2019_02_20.csv', 
  Bucket='gid-staging',
  # During upload, set ACL to public-read
  ExtraArgs = {
    'ACL': 'public-read'})
```

**Making multiple files public**
```python
# List only objects that start with '2019/final_'
response = s3.list_objects(
    Bucket='gid-staging', Prefix='2019/final_')

# Iterate over the objects
for obj in response['Contents']:
  
    # Give each object ACL of public-read
    s3.put_object_acl(Bucket='gid-staging', 
                      Key=obj['Key'], 
                      ACL='public-read')
    
    # Print the Public Object URL for each object
    print("https://{}.s3.amazonaws.com/{}".format('gid-staging', obj['Key']))
```

## 2.2. Accessing private objects in S3

If we are **forbidden** to download a private file using pandas:
```python
df = pd.read_csv('https://gid-staging.s3.amazonaws.com/potholes.csv')
```
We can use boto3 `download_file` method to download it and read it from the disk 
```python
s3.download_file( 
    Filename='potholes_local.csv', 
    Bucket='gid-staging', 
    Key='2019/potholes_private.csv')

pd.read_csv('./potholes_local.csv')
```

This is a great option if we expect this file not to change much. We have better way.
**Accessing private files**
**Use `'get_object'`**
```python
obj = s3.get_object(Bucket='gid-requests', Key='2019/potholes.csv') 
print(obj)
```
<img src="/assets/images/20210505_AWSBoto/pic17.png" class="largepic"/>

* We get a Body key, with a "StreamingBody" object as the value. A StreamingBody is a special type of response that doesn't download the whole object immediately.
* Pandas knows how to handle this type of response. We pass the contents of this key to pandas, and it will read it like a CSV file.

Read `StreamingBody` into Pandas
```python
pd.read_csv(obj['Body'])
```

**Pre-signed URLs**
* Expire after a certain timeframe 
* Great for temporary access

**Generate Presigned URL**
```python
share_url = s3.generate_presigned_url( 
    ClientMethod='get_object', 
    ExpiresIn=3600,
    Params={'Bucket': 'gid-requests','Key': 'potholes.csv'}
)

# Open in Pandas
pd.read_csv(share_url)
```
We generate a pre-signed URL that grants access to the file for 1 hour or 3600 seconds. This way, our colleague can open it in pandas or a browser. After 1 hour passes, the access expires.

**Load multiple files into one DataFrame**
```python
# Create list to hold our DataFrames 
df_list = []

# Request the list of csv's from S3 with prefix; Get contents 
response = s3.list_objects(
    Bucket='gid-requests', 
    Prefix='2019/')

# Get response contents 
request_files = response['Contents']

# Iterate over each object 
for file in request_files:
    obj = s3.get_object(Bucket='gid-requests', Key=file['Key'])

    # Read it as DataFrame
    obj_df = pd.read_csv(obj['Body'])

    # Append DataFrame to list 
    df_list.append(obj_df)

# Concatenate all the DataFrames in the list
df = pd.concat(df_list)

# Preview the DataFrame
df.head()
```

<img src="/assets/images/20210505_AWSBoto/pic18.png" class="largepic"/>

**Practice: Opening a private file**
```python
df_list =  [ ] 

for file in response['Contents']:
    # For each file in response load the object from S3
    obj = s3.get_object(Bucket='gid-requests', Key=file['Key'])
    # Load the object's StreamingBody with pandas
    obj_df = pd.read_csv(obj['Body'])
    # Append the resulting DataFrame to list
    df_list.append(obj_df)

# Concat all the DataFrames with pandas
df = pd.concat(df_list)

# Preview the resulting DataFrame
df.head()
```

## 2.3. Sharing files through a website

**Serving HTML**
Many python data science tools have the ability to generate simple HTML from data. The goal is not to have us making beautiful websites, but to share our analyses with the world.

**Convert DataFrame to html**
```python
df.to_html('table_agg.html',render_link=True)
```

<img src="/assets/images/20210505_AWSBoto/pic19.png" class="largepic"/>

The optional `render_link` argument converts any URLs to be clickable.

**Column**

We can use the columns parameter to pass a list of only columns we want rendered. In this example, we are only showing service_name and info_link columns.

<img src="/assets/images/20210505_AWSBoto/pic20.png" class="largepic"/>

**Border**
```python
df.to_html('table_agg.html',
            render_links=True,
            columns['service_name', 'request_count', 'info_link'], 
            border=0)

```
<img src="/assets/images/20210505_AWSBoto/pic21.png" class="largepic"/>

**Uploading HTML file to S3**

```python
s3.upload_file( 
    Filename='./table_agg.html', 
    Bucket='datacamp-website', 
    Key='table.html',
    ExtraArgs = {
        'ContentType': 'text/html', 
        'ACL': 'public-read'}
)
```
Accessing HTML file
```
https://{bucket}.s3.amazonaws.com/{key}
https://datacamp-website.s3.amazonaws.com/table.html
```
<img src="/assets/images/20210505_AWSBoto/pic22.png" class="largepic"/>

**Uploading other types of contents** 
```python
s3.upload_file( 
    Filename='./plot_image.png', 
    Bucket='datacamp-website', 
    Key='plot_image.png', 
    ExtraArgs = {
        'ContentType': 'image/png', 
        'ACL': 'public-read'}
)
```

**IANA Media Types**
* **JSON: `application/json`**
* **PNG: `image/png`**
* **PDF: `application/pdf`**
* **CSV: `text/csv`**

**Generating an index page**

We have multiple HTML and image files that we want to share, and we want to generate a listing for them

```python
# List the gid-reports bucket objects starting with 2019/
r = s3.list_objects(Bucket='gid-reports', Prefix='2019/')

# Convert the response contents to DataFrame
objects_df = pd.DataFrame(r['Contents'])

# Create a column "Link" that contains website url + key 
base_url = "http://datacamp-website.s3.amazonaws.com/" 
objects_df['Link'] = base_url + objects_df['Key']

# Write DataFrame to html 
objects_df.to_html('report_listing.html',
                    columns=['Link', 'LastModified', 'Size'], 
                    render_links=True)
```
<img src="/assets/images/20210505_AWSBoto/pic23.png" class="largepic"/>

**Uploading an index page**
```python
s3.upload_file( 
    Filename='./report_listing.html', 
    Bucket='datacamp-website', 
    Key='index.html',
    ExtraArgs = {
    'ContentType': 'text/html', 
    'ACL': 'public-read'}
)
```
**Practice**
```python
# Generate an HTML table with no border and selected columns
services_df.to_html('./services_no_border.html',
           # Keep specific columns only
           columns=['service_name', 'link'],
           # Set border
           border=0)

# Generate an html table with border and all columns.
services_df.to_html('./services_border_all_columns.html', 
           border=1)
# Upload the lines.html file to S3
s3.upload_file(Filename='lines.html', 
               # Set the bucket name
               Bucket='datacamp-public', 
               Key='index.html',
               # Configure uploaded file
               ExtraArgs = {
                 # Set proper content type
                 'ContentType':'text/html',
                 # Set proper ACL
                 'ACL': 'public-read'})

# Print the S3 Public Object URL for the new file.
print("http://{}.s3.amazonaws.com/{}".format('datacamp-public', 'index.html'))
```






# 5. Reference

1. [Introduction to AWS Boto in Python- DataCamp](https://learn.datacamp.com/courses/introduction-to-aws-boto-in-python)