---
layout: post
author: ledinhtrunghieu
title: Airflow
---


# 1. Introduction to Airflow

Introduction to the components of Apache Airflow and learn how and why we should use them.

## 1.1. Running a task in Airflow

**Data Engineering**: Taking any action involving data and turning it into a **reliable, repeatable, and maintainable** process.

A **work flow is**: 
* A set of steps to accomplish a given data engineering task. Such as: downloading files, copying data, filtering information, writing to a database, etc.
* The complexity of a workflow is completely dependent on the needs of the user
* A term with various meaning depending on context

<img src="/assets/images/20210502_AirflowPython/pic1.png" class="largepic"/>

**Airflow**: 

<img src="/assets/images/20210502_AirflowPython/pic2.png" class="largepic"/>


* A platform to program workflows (general), including the creation, scheduling, and monitoring of said workflows.
* Airflow can use various tools and languages, but the actual workflow code is written with Python.
* Airflow implements workflows as DAGs, or Directed Acyclic Graphs. 
* Airflow can be accessed and controlled via code, via the command-line, or via a built-in web interface. 

**Other workflow**
* Luigi
* SSIS
* Bash Scripting

**Directed Acyclic Graph**

<img src="/assets/images/20210502_AirflowPython/pic3.png" class="largepic"/>

* In Airflow, this represents the set of tasks that make up your workflow. 
* It consists of the tasks and the dependencies between tasks. 
* DAGs are created with various details about the DAG, including the name, start date, owner, email alerting options, etc.

```python
    etl_dag = DAG( dag_id='etl_pipeline',
                   default_args={"start_date": "2020-01-08"}
)
```

**Running a workflow in Airflow**
Running a simple Airflow task
```
airflow run <dag_id> <task_id> <start_date>

airflow run eexample-etl download-file 2020-01-10
```

## 1.2. Airflow DAGs

**Directed Acyclic Graph**:
* **Directed**, meaning there is an inherent flow representing the dependencies or order between execution of components. These dependencies (even implicit ones) provide context to the tools on how to order the running of components
* **Acyclic** - it does not loop or repeat. This does not imply that the entire DAG cannot be rerun, only that the individual components are executed once per run
* **Graph** represents the components and the relationships (or dependencies) between them. 

**DAG in Airflow**
* Are written in Python
* Are made up of components (typically tasks) to be executed, such as operators, sensors, etc.  Typically refers to these as tasks.
* Airflow DAGs contain dependencies that are defined, either explicitly or implicitly. These dependencies define the execution order so Airflow knows which components should be run at what point within the workflow. For example, you would likely want to copy a file to a server prior to trying to import it to a database.

**Define a DAG**
```python
from airflow.models import DAG

from datetime import datetime 
default_arguments = {
    'owner': 'jdoe',
    'email': 'jdoe@datacamp.com', 
    'start_date': datetime(2020, 1, 20)
}

etl_dag = DAG( 'etl_workflow', default_args=default_arguments )
```
The **`start_date`** represents the earliest datetime that a DAG could be run.

**DAGs on the command line**
* The `airflow` command line program contains many subcommands. 
* `airflow -h` for descriptions
* `airflow list_dags` to show all recognized DAGs


**Use python to**
* Create a DAG
* Edit the individual properties of a DAG

**Use the command line tool to**
* Start Airflow processes
* Manually run DAGs/task
* Get logging information from Airflow


```python
# Import the DAG object
from airflow.models import DAG

# Define the default_args dictionary
default_args = {
  'owner': 'dsmith',
  'start_date': datetime(2020, 1, 14),
  'retries': 2
}

# Instantiate the DAG object to a variable called etl_dag with a DAG named example_etl.
etl_dag = DAG('example_etl', default_args=default_args)
```

## 1.3. Airflow web interface

<img src="/assets/images/20210502_AirflowPython/pic4.png" class="largepic"/>

<img src="/assets/images/20210502_AirflowPython/pic5.png" class="largepic"/>

Starting the Airflow webserver

```
airflow webserver -p 9090
```

Remember that the Airflow UI allows various methods to view the state of DAGs. The `Tree View` lists the tasks and any ordering between them in a tree structure, with the ability to compress / expand the nodes. The `Graph View` shows any tasks and their dependencies in a graph structure, along with the ability to access further details about task runs. The Code view provides full access to the Python code that makes up the DAG.


# 2. Implementing Airflow DAGs

Learn the basics of implementing Airflow DAGs, how to set up and deploy operators, tasks, and scheduling.

## 2.1. Airflow operators

**Operator**
* Airflow operators represent a single task in a workflow. This can be any type of task from running a command, sending an email, running a Python script,...
* Airflow operators run independently - meaning that all resources needed to complete the task are contained within the operator.
* Airflow operators do not share information between each other. This is to simplify workflows and allow Airflow to run the tasks in the most efficient manner. It is possible to share information between operators.
* Airflow contains many various operators to perform different tasks. For example, the DummyOperator can be used to represent a task for troubleshooting or a task that has not yet been implemented
```python
DummyOperator(task_id='example', dag=dag)
```

**BashOperator**
```bash
BashOperator(
task_id='bash_example', bash_command='echo "Example!"', dag=ml_dag)
```
```bash
BashOperator(
task_id='bash_script_example', bash_command='runcleanup.sh', dag=ml_dag)
```
* Executes a given Bash command or script.
* Runs the command in a temporary directory.
* Can specify environment variables for the command.

**Example**
```py
from airflow.operators.bash_operator import BashOperator 

example_task = BashOperator(task_id='bash_ex',
                            bash_command='echo 1', 
                            dag=dag)

bash_task = BashOperator(task_id='clean_addresses', 
                         bash_command='cat addresses.txt | awk "NF==10" > cleaned.txt', 
                         dag=dag)
```

```python
# Import the BashOperator
from airflow.operators.bash_operator import BashOperator

# Define the BashOperator 
cleanup = BashOperator(
    task_id='cleanup_task',
    # Define the bash_command
    bash_command='cleanup.sh',
    # Add the task to the dag
    dag=analytics_dag
)
```
```python
# Define a second operator to run the `consolidate_data.sh` script
consolidate = BashOperator(
    task_id='consolidate_task',
    bash_command='consolidate_data.sh',
    dag=analytics_dag)

# Define a final operator to execute the `push_data.sh` script
push_data = BashOperator(
    task_id='pushdata_task',
    bash_command='push_data.sh',
    dag=analytics_dag)
```


**Note**
* Individual operators are not guaranteed to run in the same location or environment. This means that just because one operator ran in a given directory with a certain setup, it does not necessarily mean that the next operator will have access to that same information.
* You may need to set up environment variables, especially for the BashOperator. For example, it's common in bash to use the tilde character to represent a home directory. This is not defined by default in Airflow. Another example of an environment variable could be AWS credentials, database connectivity details, or other information specific to running a script.
* It can also be tricky to run tasks with any form of elevated privilege. This means that any access to resources must be setup for the specific user running the tasks. If you're uncertain what elevated privileges are, think of running a command as root or the administrator on a system.

## 2.2. Airflow tasks

**Tasks** are:
* Instances of operators
* Usually assigned to a variable in Python
```
example_task = BashOperator(task_id='bash_example',bash_command='echo "Example!"', dag=dag)
```
* Referred to by the task_id within the Airflow tools

**Task dependencies**
* Define a given order of task completion
* Are not required for a given workflow, but usually present in most 
* Are referred to as *upstream* or *downstream* tasks. An upstream task means that it must complete prior to any downstream tasks.
* In Air ow 1.8 and later, are defined using the bitshift operators
  * **>>** or the upstream operator
  * **<<** or the downstream operator
* Upstream means before and downstream means after. This means that any upstream tasks would need to complete prior to any downstream ones.

**Example**

```python
 Define the tasks
task1 = BashOperator(task_id='first_task',
                     bash_command='echo 1', 
                     dag=example_dag)

task2 = BashOperator(task_id='second_task',
                     bash_command='echo 2', 
                     dag=example_dag)

# Set first_task to run before second_task 
task1 >> task2	# or task2 << task1
```

**Task dependencies in the Airflow UI**

<img src="/assets/images/20210502_AirflowPython/pic6.png" class="largepic"/>

**Multiple dependencies**

**Chained dependencies**

```py
task1 >> task2 >> task3 >> task4
```
<img src="/assets/images/20210502_AirflowPython/pic7.png" class="largepic"/>

**Mixed dependencies**

```py
task1 >> task2 << task3
```

```py
task1	>>	task2
task3	>>	task2
```
<img src="/assets/images/20210502_AirflowPython/pic8.png" class="largepic"/>

```python
# Define a new pull_sales task
pull_sales = BashOperator(
    task_id='pullsales_task',
    bash_command='wget https://salestracking/latestinfo?json',
    dag=analytics_dag
)

# Set pull_sales to run prior to cleanup
pull_sales >> cleanup

# Configure consolidate to run after cleanup
consolidate << cleanup

# Set push_data to run last
consolidate >> push_data

```
## 2.3. Additional operators

**Python Operators**
* Executes a Python function / callable
* Operates similarly to the BashOperator, with more options 
* Can pass in arguments to the Python code

```py
from airflow.operators.python_operator import PythonOperator

def printme():
    print("This goes in the logs!") 
python_task = PythonOperator(
    task_id='simple_print', 
    python_callable=printme, 
    dag=example_dag
)
```

**Arguments**
* Supports arguments to tasks
  * Positional
  * Keyword
* Use the `op_kwargs` dictionary

**op_kwargs Example**
```py
def sleep(length_of_time): 
    time.sleep(length_of_time)

sleep_task = PythonOperator( 
    task_id='sleep', 
    python_callable=sleep, 
    op_kwargs={'length_of_time': 5} 
    dag=example_dag
)
```

We'll add our `op_kwargs` dictionary with the length of time variable and the value of 5. Note that the dictionary key must match the name of the function argument


**Email Operator**
* Found in the `airflow.operators` library
* Send an email 
* Can contain typical comments
  * HTML content
  * Attachments
* Does require the Airflow system to be configured with email server details

```py
from airflow.operators.email_operator import EmailOperator

email_task = EmailOperator( 
    task_id='email_sales_report', 
    to='sales_manager@example.com', 
    subject='Automated Sales Report',
    html_content='Attached is the latest sales report', 
    files='latest_sales.xlsx',
    dag=example_dag
)
```

**Using the PythonOperator Practice**
```py
def pull_file(URL, savepath):
    r = requests.get(URL)
    with open(savepath, 'wb') as f:
        f.write(r.content)   
    # Use the print method for logging
    print(f"File pulled from {URL} and saved to {savepath}")

from airflow.operators.python_operator import PythonOperator

# Create the task
pull_file_task = PythonOperator(
    task_id='pull_file',
    # Add the callable
    python_callable=pull_file,
    # Define the arguments
    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},
    dag=process_sales_dag
)
```

**Email Operators**
```py
# Import the Operator
from airflow.operators.email_operator import EmailOperator


# Define the task
email_manager_task = EmailOperator(
    task_id='email_manager',
    to='manager@datacamp.com',
    subject='Latest sales JSON',
    html_content='Attached is the latest sales JSON file as requested.',
    files='parsedfile.json',
    dag=process_sales_dag
)

# Set the order of tasks
pull_file_task >> parse_file_task >> email_manager_task
```

## 2.4. Airflow scheduling

**DAG Runs**
* A specific instance of a work ow at a point in time 
* Can be run manually or via `schedule_interval`
* Maintain state for each workflow and the tasks within
  * `running`
  * `failed`
  * `sucess`

**DAG Runs View**
<img src="/assets/images/20210502_AirflowPython/pic9.png" class="largepic"/>


**Schedule details**
When scheduling a DAG, there are several attributes of note:
* `start_date` - The date / time to initially schedule the DAG run
* `end_date` - Optional attribute for when to stop running new DAG instances
* `max_tries` - Optional attribute for how many times to retry before fully failing the DAG run
* `schedule_interval` - How often to schedule the DAG for execution

**Schedule interval**
* How often to schedule the DAG
* Occurs between the `start_date` and the potential `end_date`
* Can be defined with a `cron` style syntax or via built-in presets.

**cron syntax**
<img src="/assets/images/20210502_AirflowPython/pic10.png" class="largepic"/>

* Is pulled from the Unix cron format 
* Consists of 5 fields separated by a space. Starting with the minute value (0 through 59), the hour (0 through 23), the day of the month (1 through 31), the month (1 through 12), and the day of week (0 through 6).
* An asterisk `*` represents running for every interval (ie, every minute, every day, etc)
* Can be comma separated values in fields for a list of values. For example, an asterisk in the minute field means run every minute
* A list of values can be given on a field via comma separated values.

**cron example**
```
0 12 * * *                  # Run daily at noon

* * 25 2 *                  # Run once per minute on February 25

0,15,30,45 * * * *          # Run every 15 minutes
```


**Airflow scheduler presets**
Preset:
* @hourly : `0 * * * *`
* @daily : `0 0 * * *`
* @weekly: `0 0 * * 0`
* @monthly: `0 0 1 * *`
* @yearly: `0 0 1 1 *`

**Special present**
* `None`- Don't schedule ever, used for manually triggered DAGs
* `@once` - Schedule only once

**schedule_interval issues**
When scheduling a DAG, Airflow will:
* Use the `start_date` as the earliest possible value
* Schedule the task at `start_date` + `schedule_interval`
```py
'start_date': datetime(2020, 2, 25), 
'schedule_interval': @daily
```
This means the earliest starting time to run the DAG is on February 26th, 2020

```py
# Update the scheduling arguments as defined
default_args = {
  'owner': 'Engineering',
  'start_date': datetime(2019,  11, 1),
  'email': ['airflowresults@datacamp.com'],
  'email_on_failure': False,
  'email_on_retry': False,
  'retries': 3,
  'retry_delay': timedelta(minutes=20)
}

dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')
```

# 3. Maintaining and monitoring Airflow workflows

Save time using Airflow components such as sensors and executors while monitoring and troubleshooting Airflow workflows.

# 3.1. Airflow sensors

**Sensor**
* An operator that waits for a certain condition to be true
  * Creation of a file
  * Upload of a database record
  * Certain response from a web request
* Can define how often to check for the condition to be true
* Are assigned to tasks

**Sensor details**
* Derived from `airflow.sensors.base_sensor_operator`
* Sensor arguments:
* `mode`- How to check for the condition
  * `mode='poke'` - The default is poke, and means to continue checking until complete without giving up a worker slot.
  * `mode='reschedule'` - Give up task slot and try again later
* `poke_interval` -	How often to wait between checks. 
* `timeout` - How long to wait before failing task
* Also includes normal operator attributes: `task_id` and `dag`

**File Sensor**
* Is part of the `airflow.contrib.sensors` library
* Checks for the existence of a file at a certain location
* Can also check if any files exist within a directory

```py
from airflow.contrib.sensors.file_sensor import FileSensor

file_sensor_task = FileSensor(task_id='file_sense',
                              filepath='salesdata.csv', 
                              poke_interval=300, 
                              dag=sales_report_dag)

init_sales_cleanup >> file_sensor_task >> generate_report

```
**Other sensors**
* `ExternalTaskSensor` - wait for a task in another DAG to complete
* `HttpSensor` - - Request a web URL and check for content
* `SqlSensor` - Runs a SQL query to check for content
* Many others in `airflow.sensors` and `airflow.contrib.sensors`

**Sensors vs Operators**
Use a sensor when:
* Uncertain when conditions will be true
* If you want to continue to check for a condition but not necessarily fail the entire DAG immediately.
* If you want to repeatedly run a check without adding cycles to your DAG, sensors are a good choice.

<img src="/assets/images/20210502_AirflowPython/pic22.png" class="largepic"/>

# 3.2. Airflow executors

**Executors**
* Executors run tasks
* Different executors handle running the tasks differently
* Example executors:
  * `SequentialExecutors`
  * `LocalExecutor`
  * `CeleryExecutor`

**Sequential Executors**
* The default Airflow executor
* Runs one task at a time. This means having multiple workflows scheduled around the same timeframe may cause things to take longer than expected
* Useful for debugging
* While functional, not really recommended for production due to the limitations of task

**Local Executor**
* Runs entirely on a single system. 
* Treats tasks as processes on the local system
* Able to start as many concurrent tasks as desired / requested / and permitted by the system resources (ie, CPU cores, memory, etc). 
* This concurrency is the parallelism of the system, and it is defined by the user in one of two ways - either unlimited, or limited to a certain number of simultaneous tasks.
* Is a good choice for a single production Airflow system and can utilize all resources of a given host system

**Celery Executor**
* General queuing system written in Python that allows multiple systems to communicate as a basic cluster.
* Uses a Celery backend as task manager
* Significantly more difficult to setup and configure
* It requires a working Celery configuration prior to configuring Airflow, not to mention some method to share DAGs between the systems (ie, a git server, Network File System, etc).
* Powerful choice for anyoneWorking with a large number of DAGs and / or expects their processing needs to grow.

**Determine your executor**
* Via the `airflow.cfg` file
* Look for the `executor=` line
<img src="/assets/images/20210502_AirflowPython/pic11.png" class="largepic"/>
* Via the first line of `airflow list_dags`
`INFO - Using SequentialExecutor`
<img src="/assets/images/20210502_AirflowPython/pic12.png" class="largepic"/>

```py
from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.contrib.sensors.file_sensor import FileSensor
from datetime import datetime

report_dag = DAG(
    dag_id = 'execute_report',
    schedule_interval = "0 0 * * *"
)

precheck = FileSensor(
    task_id='check_for_datafile',
    filepath='salesdata_ready.csv',
    start_date=datetime(2020,2,20),
    mode='reschedule',
    dag=report_dag
)

generate_report_task = BashOperator(
    task_id='generate_report',
    bash_command='generate_report.sh',
    start_date=datetime(2020,2,20),
    dag=report_dag
)

precheck >> generate_report_task
```

## 3.3. Debugging and troubleshooting in Airflow

**Typical issues...**
* DAG won't run on schedule
* DAG won't load
* Syntax errors

**DAG won't run on schedule**
* Check if scheduler is running
<img src="/assets/images/20210502_AirflowPython/pic13.png" class="largepic"/>
* Fix by running `airflow scheduler` from the command line
* At least one `scheduler_interval` hasn't passed from the `start_date` or last DAG run
  * Modify the attributes to meet your requirements
* The executor does not have enough free slots to run tasks.
  * Changing the executor type to something capable of more tasks (LocalExecutor or CeleryExecutor)
  * Adding more systems or system resources (RAM, CPUs)
  * Changing the scheduling of your DAGs.

**DAG won't load**
* DAG not in web UI 
* DAG not in `airflow list_dags`
* Possible solution:
  * Verify DAG le is in correct folder 
  * Determine the DAGs folder via `airflow.cfg`
  * The folder must be an absolute path
<img src="/assets/images/20210502_AirflowPython/pic14.png" class="largepic"/>

**Syntex error**
* The most common reason a DAG le won't appear
* Sometimes difficult to find errors in DAG
* Two quick methods: 
  * Run `airflow list_dags`
<img src="/assets/images/20210502_AirflowPython/pic15.png" class="largepic"/>
  * Run `python3 <dagfile.py>
    * With Errors:
<img src="/assets/images/20210502_AirflowPython/pic16.png" class="largepic"/>
    * Without Errors: Return to command line
<img src="/assets/images/20210502_AirflowPython/pic17.png" class="largepic"/>


## 3.4. SLAs and reporting in Airflow

**SLAs**
* An **SLA** stands for **Service Level Agreement**
* Within Airflow, the amount of time a task or a DAG should require to run. 
* An **SLA Miss** is any time the task / DAG does not meet the expected timing
* If an SLA is missed, an email is sent out and a log is stored.
* You can view SLA misses in the web UI.

**SLA Misses**
* Found under Browse: SLA Misses. It provides you general information about what task missed the SLA and when it failed. It also indicates if an email has been sent when the SLA failed.
<img src="/assets/images/20210502_AirflowPython/pic18.png" class="largepic"/>

**Defining SLAs**
* Using the `'sla'` argument on the task
```py
task1 = BashOperator(task_id='sla_task',
                     bash_command='runcode.sh', 
                     sla=timedelta(seconds=30), 
                     dag=dag
```
* On the `default_args` dictionary
```py
default_args={
  'sla': timedelta(minutes=20) 
  'start_date': datetime(2020,2,20)
}
dag = DAG('sla_dag', default_args=default_args)
```

**timedelta object**
* In the `date_time` library
* Accessed via `from datetime import timedelta`
* Takes arguments of days, seconds, minutes, hours, and weeks
```py
timedelta(seconds=30) 
timedelta(weeks=2)
timedelta(days=4, hours=10, minutes=20, seconds=30)
```

**General reporting**
* Options for success / failure / error
* Keys in the `default_args` dictionary
```py
default_args={
  'email': ['airflowalerts@datacamp.com'], 
  'email_on_failure': True, 
  'email_on_retry': False, 
  'email_on_success': True,
...
}
```
* Within DAGs from the EmailOperator

**Defining an SLA Example**
You've successfully implemented several Airflow workflows into production, but you **don't currently have any method of determining if a workflow takes too long to run**. After consulting with your manager and your team, you decide to implement an **SLA** at the DAG level on a test workflow.
* Import the timedelta object.
* Define an SLA of 30 minutes.
* Add the SLA to the DAG.

```py
# Import the timedelta object
from datetime import timedelta

# Create the dictionary entry
default_args = {
  'start_date': datetime(2020, 2, 20),
  'sla': timedelta(minutes=30)
}

# Add to the DAG
test_dag = DAG('test_workflow', default_args=default_args, schedule_interval='@None')
```

After completing the SLA on the entire workflow, you realize you really only need the SLA timing on a specific task instead of the full workflow.
```py
# Import the timedelta object
from datetime import timedelta

test_dag = DAG('test_workflow', start_date=datetime(2020,2,20), schedule_interval='@None')

# Create the task with the SLA
task1 = BashOperator(task_id='first_task',
                     sla=timedelta(hours=3),
                     bash_command='initialize_data.sh',
                     dag=test_dag)
```

**Generate and email a report**
```py
# Define the email task
email_report = EmailOperator(
        task_id='email_report',
        to='airflow@datacamp.com',
        subject='Airflow Monthly Report',
        html_content="""Attached is your monthly workflow report - please refer to it for more detail""",
        files=['monthly_report.pdf'],
        dag=report_dag
)

# Set the email task to run after the report is generated
email_report << generate_report
```

**Adding status emails**
```py
from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.contrib.sensors.file_sensor import FileSensor
from datetime import datetime

default_args={
    'email': ['airflowalerts@datacamp.com','airflowadmin@datacamp.com'], 
    'email_on_failure': True,
    'email_on_success': True,
}

report_dag = DAG(
    dag_id = 'execute_report',
    schedule_interval = "0 0 * * *",
    default_args=default_args
)

precheck = FileSensor(
    task_id='check_for_datafile',
    filepath='salesdata_ready.csv',
    start_date=datetime(2020,2,20),
    mode='reschedule',
    dag=report_dag)

generate_report_task = BashOperator(
    task_id='generate_report',
    bash_command='generate_report.sh',
    start_date=datetime(2020,2,20),
    dag=report_dag
)

precheck >> generate_report_task
```

# 4. Building production pipelines in Airflow

Build a production-quality workflow in Airflow.

## 4.1. Working with templates

**Templates**
* Allow substituting information during a DAG run. Every time a DAG with templated information is executed, information is interpreted and included with the DAG run. 
* Provide added exibility when defining tasks
* Are created using the `Jinja` templating language

**Non-Templated BashOperator example**
Create a task to echo a list of files:
```py
t1 = BashOperator(
      task_id='first_task',
      bash_command='echo "Reading file1.txt"', 
      dag=dag)
t2 = BashOperator(
      task_id='second_task', 
      bash_command='echo "Reading file2.txt"', 
      dag=dag)
```
Consider what this would look like if we had 5, 10, or even 100+ files we needed to process. 

**Templated BashOperator example**
```py
templated_command="""
  echo "Reading {{ params.filename }}" 
"""
t1 = BashOperator(
      task_id='template_task', 
      bash_command=templated_command, 
      params={'filename': 'file1.txt'} 
      =example_dag)
```

Output:
```py
Reading file1.txt
```

```py
templated_command="""
  echo "Reading {{ params.filename }}" 
"""

t1 = BashOperator(task_id='template_task', bash_command=templated_command, params={'filename': 'file1.txt'} dag=example_dag)
t2 = BashOperator(task_id='template_task', bash_command=templated_command, params={'filename': 'file2.txt'} dag=example_dag)
```

**Creating a templated BashOperator**
```py
from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime

default_args = {
  'start_date': datetime(2020, 4, 15),
}

cleandata_dag = DAG('cleandata',
                    default_args=default_args,
                    schedule_interval='@daily')

# Create a templated command to execute
# 'bash cleandata.sh datestring'
templated_command = """
bash cleandata.sh {{ ds_nodash }}
"""

# Modify clean_task to use the templated command
clean_task = BashOperator(task_id='cleandata_task',
                          bash_command=templated_command,
                          dag=cleandata_dag)
```

```py
from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime

default_args = {
  'start_date': datetime(2020, 4, 15),
}

cleandata_dag = DAG('cleandata',
                    default_args=default_args,
                    schedule_interval='@daily')

# Modify the templated command to handle a
# second argument called filename.
templated_command = """
  bash cleandata.sh {{ ds_nodash }} {{ params.filename }}
"""

# Modify clean_task to pass the new argument
clean_task = BashOperator(task_id='cleandata_task',
                          bash_command=templated_command,
                          params={'filename': 'salesdata.txt'},
                          dag=cleandata_dag)
                          
# Create a new BashOperator clean_task2
clean_task2 = BashOperator(task_id='cleandata_task2',
                           bash_command=templated_command,
                           params={'filename': 'supportdata.txt'},
                           dag=cleandata_dag)
                           
# Set the operator dependencies
clean_task >> clean_task2
```

## 4.2. Advanced template
```py
templated_command="""
{% for filename in params.filenames %} 
  echo "Reading {{ filename }}"
{% endfor %} 
"""
t1 = BashOperator(task_id='template_task', 
                  bash_command=templated_command, 
                  params={'filenames': ['file1.txt', 'file2.txt']} 
                  dag=example_dag)
```
```
Reading file1.txt 
Reading file2.txt
```

**Variables**
* Airflow built-in runtime variables
* Provides assorted information about DAG runs, tasks, and even the system configuration. 
* Examples include:
```py
Execution Date: {{ ds }}		#	YYYY-MM-DD
Execution Date, no dashes: {{ ds_nodash }}		#	YYYYMMDD
Previous Execution date: {{ prev_ds }}		#	YYYY-MM-DD
Prev Execution date, no dashes: {{ prev_ds_nodash	}}	#	YYYYMMDD
DAG object: {{ dag }}
Airflow config object: {{ conf }}
```

**Macros**
In addition to others, there is also a `{{ macros }}` variable.
This is a reference to the Airflow macros package which provides various useful objects / methods for Airflow templates.
* `{{ macros.datetime }}` : The `datetime.datetime` object
* `{{ macros.timedelta }}`: The `timedelta` object
* `{{ macros.uuid }}`: Python's `uuid` object
* `{{ macros.ds_add('2020-04-15', 5) }}`: Modify days from a date, this example returns 2020-04-20

**Using lists with templates**
```py
from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime

filelist = [f'file{x}.txt' for x in range(30)]

default_args = {
  'start_date': datetime(2020, 4, 15),
}

cleandata_dag = DAG('cleandata',
                    default_args=default_args,
                    schedule_interval='@daily')

# Modify the template to handle multiple files in a 
# single run.
templated_command = """
  <% for filename in params.filenames %>
  bash cleandata.sh {{ ds_nodash }} {{ filename }};
  <% endfor %>
"""

# Modify clean_task to use the templated command
clean_task = BashOperator(task_id='cleandata_task',
                          bash_command=templated_command,
                          params={'filenames': filelist},
                          dag=cleandata_dag)
                          
```

**Sending templated emails**
```py
from airflow.models import DAG
from airflow.operators.email_operator import EmailOperator
from datetime import datetime

# Create the string representing the html email content
html_email_str = """
Date: {{ ds }}
Username: {{ params.username }}
"""

email_dag = DAG('template_email_test',
                default_args={'start_date': datetime(2020, 4, 15)},
                schedule_interval='@weekly')
                
email_task = EmailOperator(task_id='email_task',
                           to='testuser@datacamp.com',
                           subject="{{ macros.uuid.uuid4() }}",
                           html_content=html_email_str,
                           params={'username': 'testemailuser'},
                           dag=email_dag)
```

## 4.3. Branching

**Branching in Airflow:**
* Provides conditional logic. This means that tasks can be selectively executed or skipped depending on the result of an Operator
* Using `BranchPythonOperator`
* `from airflow.operators.python_operator import BranchPythonOperator`
* Takes a `python_callable` to return the next task id (or list of ids) to follow

**Braching Example**
```py
def branch_test(**kwargs):
  if int(kwargs['ds_nodash']) % 2 == 0: 
    return 'even_day_task'
  else:
    return 'odd_day_task'

branch_task = BranchPythonOperator(task_id='branch_task',dag=dag, 
                                   provide_context=True,
                                   python_callable=branch_test)

start_task >> branch_task >> even_day_task >> even_day_task2 
branch_task >> odd_day_task >> odd_day_task2
```

We pass in the `provide_context` argument and set it to True. This is the component that tells Airflow to provide access to the runtime variables and macros to the function. 

**Braching graph view**
<img src="/assets/images/20210502_AirflowPython/pic19.png" class="largepic"/>

**Define a BranchPythonOperator**
```py
# Create a function to determine if years are different
def year_check(**kwargs):
    current_year = int(kwargs['ds_nodash'][0:4])
    previous_year = int(kwargs['prev_ds_nodash'][0:4])
    if current_year == previous_year:
        return 'current_year_task'
    else:
        return 'new_year_task'

# Define the BranchPythonOperator
branch_task = BranchPythonOperator(task_id='branch_task', dag=branch_dag,
                                   python_callable=year_check, provide_context=True)
# Define the dependencies
branch_dag >> current_year_task
branch_dag >> new_year_task
```

## 4.4. Creating a production pipeline

**Reminders**

**Running a DAGs&Tasks**
To run a specific task from command-line:
```bash
airflow run <dag_id> <task_id> <date>
```
To run a full DAG:
```bash
airflow trigger_dag -e <date> <dag_id>
```

**Operators reminder**
* BashOperator - expects a `bash_command`
* PythonOperator - excepts a `python_callable`
* BranchPythonOperator - requires a `python_callable`and `provide_context=True`. The callable must accept `**kwargs`
* FileSensor - requires `filepath` argument and might need `mode` or `poke_interval` attributes

**Template reminders**
* Many objects in Airflow can use templates
* Certain fields may use templated strings, while others do not 
* One way to check is to use built-in documentation:
1. Open python3 interpreter
2. Import necessary libraries (ie, `from airflow.operators.bash_operator import BashOperator`)
3. At prompt, run `help(<Airflow object>)`, ie, `help(BashOperator)`
4. Look for a line that referencing `template_ elds`. This will specify any of the arguments that can use templates
<img src="/assets/images/20210502_AirflowPython/pic20.png" class="largepic"/>
<img src="/assets/images/20210502_AirflowPython/pic21.png" class="largepic"/>

**Creating a production pipeline**
```py
from airflow.models import DAG
from airflow.contrib.sensors.file_sensor import FileSensor

# Import the needed operators
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from datetime import date, datetime

def process_data(**context):
  file = open('/home/repl/workspace/processed_data.tmp', 'w')
  file.write(f'Data processed on {date.today()}')
  file.close()

    
dag = DAG(dag_id='etl_update', default_args={'start_date': datetime(2020,4,1)})

sensor = FileSensor(task_id='sense_file', 
                    filepath='/home/repl/workspace/startprocess.txt',
                    poke_interval=5,
                    timeout=15,
                    dag=dag)

bash_task = BashOperator(task_id='cleanup_tempfiles', 
                         bash_command='rm -f /home/repl/*.tmp',
                         dag=dag)

python_task = PythonOperator(task_id='run_processing', 
                             python_callable=process_data,
                             dag=dag)

sensor >> bash_task >> python_task
```

```py
from airflow.models import DAG
from airflow.contrib.sensors.file_sensor import FileSensor
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from dags.process import process_data
from datetime import timedelta, datetime

# Update the default arguments and apply them to the DAG
default_args = {
  'start_date': datetime(2019,1,1),
  'sla': timedelta(minutes=90) 
}

dag = DAG(dag_id='etl_update', default_args=default_args)

sensor = FileSensor(task_id='sense_file', 
                    filepath='/home/repl/workspace/startprocess.txt',
                    poke_interval=45,
                    dag=dag)

bash_task = BashOperator(task_id='cleanup_tempfiles', 
                         bash_command='rm -f /home/repl/*.tmp',
                         dag=dag)

python_task = PythonOperator(task_id='run_processing', 
                             python_callable=process_data,
                             provide_context = True,
                             dag=dag)

sensor >> bash_task >> python_task
```py
from airflow.models import DAG
from airflow.contrib.sensors.file_sensor import FileSensor
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.operators.python_operator import BranchPythonOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.email_operator import EmailOperator
from dags.process import process_data
from datetime import datetime, timedelta

# Update the default arguments and apply them to the DAG.

default_args = {
  'start_date': datetime(2019,1,1),
  'sla': timedelta(minutes=90)
}
    
dag = DAG(dag_id='etl_update', default_args=default_args)

sensor = FileSensor(task_id='sense_file', 
                    filepath='/home/repl/workspace/startprocess.txt',
                    poke_interval=45,
                    dag=dag)

bash_task = BashOperator(task_id='cleanup_tempfiles', 
                         bash_command='rm -f /home/repl/*.tmp',
                         dag=dag)

python_task = PythonOperator(task_id='run_processing', 
                             python_callable=process_data,
                             provide_context=True,
                             dag=dag)


email_subject="""
  Email report for {{ params.department }} on {{ ds_nodash }}
"""


email_report_task = EmailOperator(task_id='email_report_task',
                                  to='sales@mycompany.com',
                                  subject=email_subject,
                                  html_content='',
                                  params={'department': 'Data subscription services'},
                                  dag=dag)


no_email_task = DummyOperator(task_id='no_email_task', dag=dag)


def check_weekend(**kwargs):
    dt = datetime.strptime(kwargs['execution_date'],"%Y-%m-%d")
    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.
    if (dt.weekday() < 5):
        return 'email_report_task'
    else:
        return 'no_email_task'
    
    
branch_task = BranchPythonOperator(task_id='check_if_weekend',
                                   python_callable=check_weekend, 
                                   provide_context=True,
                                   dag=dag)

    
sensor >> bash_task >> python_task

python_task >> branch_task >> [email_report_task, no_email_task]

```


