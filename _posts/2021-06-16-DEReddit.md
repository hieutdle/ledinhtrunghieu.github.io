---
layout: post
author: ledinhtrunghieu
title: Remarkable Posts in Data Engineering SubReddit
---

# 1. Help

**[Is it possible to create a real time data pipeline that collects and uploads data to a database?](https://www.reddit.com/r/dataengineering/comments/o0momn/is_it_possible_to_create_a_real_time_data/)**

**Questions:**
Hi, everyone. I'm trying to work on a project in my free time. I want to create a data pipeline that uploads a file from a directory to a database. I was thinking of creating a REST API connected to PostgreSQL however I'm not sure if an API can carry out this task. My question is, is it possible to achieve this with a REST API that autmatically uploads the data to a data base and maybe records the time the upload is carried out? Thank you in advance

**Answer:**
* This is possible. You can easily achieve "near realtime" by creating the API call that loads data into the database, then using any scheduling tool to run it every N minutes. I'd usually use AWS or azure, but there's other low-overhead options out there.
* For true realtime/streaming data you should look into apache kafka. You'll want a proper processing engine to pull the data. It's much more work to configure and setup, but if you want a project to do true data streaming it could be worth it.


# 2. Discussion

**[Is Apache Spark trending down?](https://www.reddit.com/r/dataengineering/comments/o02lqu/is_apache_spark_trending_down_why/)**





# 3. Blog

# 4. Career

**[How to efficiently evaluate a candidate Python proficiency?](https://www.reddit.com/r/dataengineering/comments/o0dkpc/how_to_efficiently_evaluate_a_candidate_python/)**

**Questions:**
1. What is the difference between a tuple and a list?
2. What is a generator?
3. What is a context manager?
4. How do you manage dependencies in your Python projects?
5. What are your favorite and least favorite features of the language?
6. What is your favorite Python package and why?

**Answers:**
1. The main difference is that lists are mutable while tuples are not. Tuples send a signal to the person reading the code that the data should be static and provides some runtime safety. Tuples use less memory and are a bit faster which can make a big difference when performance is needed. Lists have more operations than tuples though so sometimes lists are easier to work with even when dealing with static data.
2. A generator is a function that can be used as a lazy iterator. This means you can use it in a for loop and have the values being iterated over generated on demand, resulting in lower memory usage and improved performance. This makes controlling memory usage much simpler in programs that need it.
3. Context managers allow you to allocate and release resources in a simple way via the "with" statement. This is useful for managing long-running connections or cleaning up temporary resources like files or directories.
4. I install dependencies with pip, manage python versions with pyenv, and keep a requirements.txt file with a list of dependencies in all my projects that are used in a setup.py script.
5. My favorite features of the language are decorators, comprehensions, generators, data classes, and context managers! They are great ways of solving common programming problems in a succinct fashion. The interpreter is also fast which makes the program start time low, which is perfect for scripting and iterating quickly. The REPL is also good and iPython notebooks are useful. My least favorite features are the lack of functional programming tools, specifically for immutable programming, the GIL, and an overall subpar concurrency model.
6. smart-open/fs-spec. I work with files in cloud storage a lot and having the same APIs for working with local files is a huge productivity gain.


# 5. Interview

**[Classic questions of SQL](https://www.reddit.com/r/dataengineering/comments/nh3yka/question_what_are_some_classic_questions_of_sql/)**
* Explain Window function
* Types of joins
* What is index?
* Does indexing the data speed up the join operation?
* If you will create index(column.a, column.b) will that speed up query select * from table x where b = 3.
* Union vs Union ALL difference


**[How to answer to data engineering case study interview questions without real world experience?](https://www.reddit.com/r/dataengineering/comments/nrv8l6/how_to_answer_to_data_engineering_case_study/)**

**Question:** How to approach answering questions like "Designing ETL to process few billions of events every day to generate report at the end of every day". Questions like what are your important design considerations and what database would you choose and why you choose that?
How to answer this kind of vague question in an structured and organized way? what are the important aspects to think of when creating ETL dealing with processing/writing billions of records? Any resources to study will be helpful. Thank you.

**Answer:**
If I were you, hearing the questions you posted, I'd ask the following:
* Are the events similar to each other or they vary a lot?
* Are the events are just time series data or they need to be cross referenced a lot?
* Similarly what kind of "reports at the end of every day" look like?
* Specifically, how many metrics we are to track and how many dimensions they can be sliced and diced by?

These questions will put a lot interviewers on defense already :) It is not hard to onboard/ingest billions of records everyday. Heck, today's technologies can handle billions per hour if you can pay up for it. Name a few - Kinesis, Kafka, Cassandra, Dynamo, Mongo, plain old file system write. Ingesting data without processing, or with scalar transformation only, is basically a Mapping only operation. It can be easily scaled out.

So the key is to go after what they mean "Processing"? If it is just simple GROUP BY based aggregation, then tell them the key is to detect and react quickly to data skewness. That means you are looking for a tool that has a modern storage engine which has the detection ability or can handle data skewness for you, e.g. by aggressively re-balancing on the fly. Basically any big data platforms except Hive would do. Since we are talking about billions of rows, so traditional RMDBS like MySQL and Postgres will be out of question too. But if they are already an Oracle shop, tell them it works too.

If Processing means a lot cross-joins among large number of events, that is harder to answer. Ask them to give you an example, press for use case: why it has to be this way? It is possible they say they want to find some common properties among two large cluster of events? It is not common -- if they hedge, they are bluffing. Once both you and them know you know that they are bluffing, you can hand-wave through the question.

However, if it is a legit question, then talk about building staged pipelines that can organize (filter, clean, summarize) those billions records step by step. A common perception is writing data to disk is expensive -- not the case in big data analysis, where data compression by column is the norm. Instead, persisting data give storage engine important clue about your data's profile, e.g. skewed or not, thus will greatly speed up subsequent queries. In other words, sometimes a TEMP table or a CTE may suffice, in big table join cases, persisting them on disk could be better.


# 6. Meme
 
# 7. Meta

Biotech is a very satisfying field to be in from a programming/Data science/data engineering side of things.

Securely storing information. Training models with anonymous data. Providing tech solutions that actually save lives/make it easier for loved ones/etc.

All the fun challenges of private work but the typically give you the extra time to make things secure and keep data private while also building a product meant to help people. Not a bad gig. Pick up some cool medical knowledge too.


https://www.reddit.com/r/cscareerquestions/comments/mykz7p/anyone_regret_wussing_out_of_other_careers_due_to/



